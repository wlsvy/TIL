# AtDevcat

## 22/08/31

처음으로 업무 일지 기록 시작합니다.

최근 서버 측 GameServer의 Channel Update가 aws ec2 인스턴스에서 실행 시 공회전 하는 현상이 있었다. 너무 상식 밖의 현상이거니와 팀장님이 체크해봤을 때 발생 원인도 구체화하지 못했다.

게임서버의 다수의 채널 업데이트는 병렬 처리된다. C# Task 에 할당되게 되는데, 어찌 된 일인지 실행되어야 할 태스크 중 일부가 놀고 있다. 게임 서버의 프레임 배리어는 모든 채널이 업데이트 되기 전에는 해당 프레임을 유지하고 다음 프레임으로 넘어가지 못하도록 막는다. 몇 개 태스크가 먹통이 되니 다른 정상적인 채널들이 배리어에 걸려 다음 프레임으로 넘어가지 못하고 있었다.

어느 때와 같은 환경이었을 텐데 갑자기 왜 이런 일이 벌어진 걸까... 결국 채널 업데이트 시에는 Task 대신 Thread를 직접 조작하는 방식으로 수정되었다. (C# ThreadPool의 Task 할당/처리 지원을 포기하고 Thread를 직접 조작하게 되면 어느 정도 성능상의 이점을 포기하는 셈이지만, 프로그래머가 직접 관리하는 수행흐름은 상태 추적이 쉽고 디버깅이 유리하다.)

## 22/09/01

프레임 배리어... 프레임 동작을 처리하는 수행 흐름이 중간에 오류가 발생해서 스택이 되감긴다. 예외를 catch한 쪽에서는 수행 흐름이 끊긴 시점이 어딘지, 프레임 배리어 안인지 밖인지 알 수 없다. 이 경우에는 어떻게 처리해야 할까??

당연히 프레임 배리어 안에서 발생할 수 있는 예외는 가능한 한 막아두었다. 그 외에 코드에서는 언뜻 보기에 예외가 발생할 수 있는 상황이 있을까 싶을 정도로 꼼꼼히 짜두었다. 정말 RAM이 꽉 차지 않는 이상 뭐가 터질 수 있나 싶을 정도로. 그렇지만 C# 을 사용하다 보면 절대 안심할 수 없다더라.

## 22/09/02

MM 서버 모니터링 도구로는 [Grafana](https://grafana.com/) 를 사용합니다.

에러 리포트 용 서버를 따로 올리고, 주기적으로 수집한 오류를 요약해서 팀즈로 보고합니다. 팀즈로 보고할 때는 [Incoming WebHook](https://docs.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook)을 사용합니다.

- [MessageCard PlayGround](https://messagecardplayground.azurewebsites.net/)

## 22/09/14

게임 실행 전에 패치 데이터 다운로드 속도가 150Mbps 를 넘지 못해서 기술실에서 잠시 소동이 있었는데, 알고보니 사내 wifi 의 최대 속도가 150Mbps로 제한되어 있었다. 인터넷 기사 아저씨가 사기 친 것이었다....

서버 프로그래밍 상에서 두려운 일 몇 가지 중에 하나는 아마 서버 프로세스의 어딘가가 잘못 되어서 디바이스의 메모리나 cpu 자원을 쥐도 새도 모르게 잠식해버리는 것이다... 그래서 디바이스 모니터링이 필수고 만약에 사태에 대비해 서버 머신을 분리하는 것이 아닐까
(사실 오늘 무한 루프가 발생하자 서버가 머신 cpu를 다 먹어버리고 드러누워버리는 현상이 있었다)

간헐적으로 실패하는 테스트. 중요한 빌드 파이프라인에서 테스트가 실패하면 노티 메세지가 개발실로 전달된다. 간헐적으로 실패하는 테스트들 그 중에서도 당장 해결하기는 까다로운 것들이 여기서 문제가 됬는데, 이런 실패 메세지를 자꾸 보게 되는 사람들이 경각심을 잃어버린다는 것이다. 신뢰를 잃어버리는 것이 가장 큰 문제다.

- 이런 테스트들은 분류를 따로 해야 하는 걸까?? '가끔 고장나는 골칫덩이들' 묶음을 만들고선 실패 횟수를 카운트해서 주기적으로 노티하는 방식은 어떨까?

## 22/09/16

밤새고 난 다음날은 업무를 거의 못했다. 커피나 에너지 드링크 좀 마시면 기운 낼 수 있지 않을까 싶은데 역부족이었다. 정말 잠이 쏟아진다.

서버 내 몬스터 생성 시스템이 고장 나서 수만 마리가 서버에 스폰되었는데, 서버 프로세스가 머신 cpu 를 100% 먹어버려서 문제가 되었다. cpu가 잠식되버려서 원격에서 서버를 강제 종료 시키는 것도 까다롭다.

## 22/09/19

오늘은 깃랩GitLab 이 느려서 많은 사람들이 불편을 느낀 날. 음... 인프라 이슈일 수 있고, 트래픽 이슈일 수 있고 ( 그날 따라 신규 입사자가 많아서 프로젝트 전체 풀 받는다던가), 프로젝트 사이즈 이슈도 고려해봄 직 하다. 아니다. 프로젝트 사이즈가 문제라면 특정한 날에 갑자기 문제가 발견되는 게 아니라 점진적으로 증상이 관측되었어야 한다. 또 뭐가 있을까? 갑자기 프로젝트에 큰 파일이 올라온다?? 이것도 아닐테지.

## 22/09/20

큰 규모의 마일스톤 작업이 마감되는 날이다. 우리 프로젝트는 메인 트렁크에 모든 작업자의 작업물이 올라오는데, 트렁크를 동결시키고 작업자 여러분들의 메인 브랜치 머지를 막고 싶지만 그게 잘 안되고 있다. ㅎ하하하. 형상 관리 작업을 지원하는 서버(merge Cat)에서 따로 동결 시기에 머지 요청을 막는 기능은 없다. (사실 있어야 할 정도로 중요한 기능이 아니긴 하다.)

## 22/09/21

MySql 에 쌓인 게임 에러리포트 1억개를 지우자. 그냥 Delete 명령어를 사용하니 너무 느리더라. 문제의 1억개 데이터 중에는 stacktrace 컬럼이 있고 가변 문자열이다. 아마 파편화가 상당히 진행됬을터다.

MySql 에서는 대량의 데이터를 갱신할 때, 데이터 사본을 먼저 만든 뒤 사본에다가 쿼리를 수행하는 방식이라 한다. 그렇다면 원본 데이터에다가 사본을 덮어씌우도록 동작할 것이다. 그러니 delete 를 사용하기는 까다로운 조건이었다. 에러리포터 서버는 상시 리포트를 받아서 DB 에 추가하기 때문에 라이트 락 경합을 피할 수 없다.

테이블 통째로 truncate 하기로 했다. drop table 과 마찬가진데 mysql 내부에서는 테이블 포인터를 비어있는 테이블로 바꿔치기 하고는 실 데이터는 나중에 차근차근 지워가는지 명령이 즉각 수행되더라. (만약에 남겨야 할 데이터가 있다면, 해당 데이터만 복사 -> 기존 테이블 truncate -> 복사한 데이터를 다시 insert 하는 방식이라면 해결됬을 것이다.)

## 22/09/22

프로젝트 용량이 200GB 에 육박한다. 빌드 대역을 여러 개 관리한다면 1TB 는 금방이다. 오늘은 빌드 머신이 용량 부족으로 드러누웠다 ㅠㅠ 매번 대역 늘릴 때마다 겪게 될 일인데 이럴 때는 해결책이 뭐가 있을까??

1. 머신 용량을 추가 확보
2. git partial clone : 과거 깃 히스토리 중 필요한 것만 선택적으로 내려받는다. 이를 테면 과거의 큰 용량의 파일이 추가되었다가 과거에 제거되었다면 해당 파일의 정보까지 현재 작업환경에서 내려받을 필요는 없을 것이다.

## 22/09/23

매일 쓸 생각은 없었는데 이슈가 하루가 멀다하고 생겨나고 있다.

aws 관련 테라폼 설정을 잘못 만졌다가 AMI(Amazon Machine Image)가 변경되었고 테라폼 코드가 관리하고 있던 전체 인스턴스에 영향이 가버렸다.

일부 인스턴스는 AMI 변경 무시 설정이 있어서 다행히 별 이상이 없었지만은 다른 일부 인스턴스는 완전히 교체되어버렸다. 내부 보관하던 데이터가 전부 날아가버린 셈.

인프라는 변경 내역을 항상 꼼꼼히 살펴야 하는 게 맞다... 사람의 손을 떠나 자동화가 처음 시작되는 곳이니 이곳에서 발생하는 휴먼에러까지 막기는 어렵겠지??

## 22/09/27

프로젝트 형상 관리 툴로 Gitlab을 사용하는데 속도가 느리다. 데브캣만 사용하는게 아니라 넥슨 포함 다른 계열사에서도 사용하는 서비스이기 때문에,

참다 못해서 gitLab 서버를 독립하기로 결정. 프로젝트 저장소를 이주한다.

![](GitlabServerExodus.png)

## 22/10/07

갤럭시의 GOS(Game Optimizing Service) 덕분에 기기들이 성능 제한 당하고 있다. 게임 플레이가 어려울 정도로 프레임이 나오지 않고 있다. 해상도/cpu 클럭/gpu 클럭/ 소리/ 밝기 까지 전부 제한된다고 하니 소비자 입장에서는 좋은 기기를 구매했어도 쾌적한 경험을 누릴 수가 없다.

[유니티에서 Samsung AdaptivePerformance를 적용하면 GOS를 해제할 수 있다는 말이 있다.](https://docs.unity3d.com/Packages/com.unity.adaptiveperformance@4.0/manual/user-guide.html)

[삼성 FAQs](https://developer.samsung.com/galaxy-gamedev/ap-userguide/faq_kor.html#:~:text=Adaptive%20Performance%20%EC%A7%80%EC%9B%90%20%EB%8B%A8%EB%A7%90%EC%9D%B4,%EB%8B%A8%EB%A7%90%EC%97%90%EC%84%9C%EB%8A%94%20%EB%8C%80%EB%B6%80%EB%B6%84%20%EC%A7%80%EC%9B%90%ED%95%A9%EB%8B%88%EB%8B%A4)

## 22/10/11

.NET이 제공하는 API로 만든 Windows Service 프로젝트에서 빌드된 프로젝트를 참조하여 진입점(main)을 직접 호출한다. 그런데 프로세스의 stdout, stderr 등이 그대로 유실되는데 이것이 문제

- 프로젝트를 WindowService 형태로 구동하면서 stdout/stderr를 유실하지 않게끔 하는 방법을 찾아야 하는데 [nssm](https://nssm.cc/) 이라는 도구가 있다. 프로세스 우아한 종료, 표준출력 로그 및 로그 파일 로테이션, 자동 재시작 등 다양한 기능을 제공한다.

## 22/10/12

유니티의 IL2CPP(Intermediate Launguage to C++)

IL은 코드를 컴파일해서 기계어에 가깝지만 특정 CPU에 의존적이지 않은 중간단계의 언어를 생성합니다만은, 이 IL은 IOS 애플리케이션에서 막혀있는 상태다. (JIT를 지원하지 않는다)

C#으로 만든 프로그램을 IOS에서 돌리려면 인터프리팅 해야해서 느리니까 유니티는 모바일 빌드를 뽑을 때 IL을 C++ 로 번역한답니다.

문제는 IL2CPP는 C# 동작을 과도하게 기계적으로 해석했다는 것이고요. 예를 들어 static 필드 초기화가 있는 클래스의 static 메서드에는 첫머리마다 '이 클래스가 초기화되어있지 않으면 초기화하자' 라는 코드가 적혀있다고 합니다.
- 쓸데없는 명령을 거치기 때문에 인스트럭션 캐시 손해를 보고
- 전역변수 포인터, 그리고 그것이 가리키는 객체의 has_ctor와 is_ctor_finished를 건드려서 캐시 미스가 납니다.

프로그램을 시작할 때 static 필드 초기화를 미리 진행한다면, 그런 코드는 필요가 없겠죠.

+ `Dictionary<int, *>` 보다 `Dictionary<string, *>` 가 20배 느리다고 하더라. 클라이언트 최적화 과정에서 지적되는 병목점
+ IL2CPP 환경을 매번 체크해야 해서 골치가 아프다.

## 22/10/13

에러리포터에는 하루 수십만 건의 리포트가 제보된다. 과거 프로젝트에서 비슷하게 센트리를 활용하던 시절에는, 들어오는 데이터 양이 많아지면 그대로 서버 유지 비용에 반영된다는데... 과거에는 이를 대비한 구현이 따로 있었던 듯 하다.

## 22/10/14

네트워크 레이턴시 대응.

크게 볼 때 두 가지 대응 방향이 있다.

1. 동기화 값 보간 (Interpolation) : 패킷은 네트워크 상황에 따라 뚝뚝 끊어져 날아올 수 있다. 비연속적인 상태를 보간해서 클라아언트 상에서는 연속적인 값 변화 상태를 보여준다.
2. 예측 (Extrapolation) : 어떤 물체가 한 방향으로 움직이고 있을 때, 클라이언트는 앞으로 짧은 시간동안 그 물체가 가던 방향 그대로 이동하고 있을 것이라고 예상할 수 있다. 클라이언트는 통계에 근거해 서버의 다음 업데이트 틱에 예상치를 사용한다.

## 22/10/17

주말에 판교 sk 데이터 센터에서 불이 나서 카카오 계열 서비스가 완전히 먹통이 되버리는 대사건이 있었다. 같은 데이터 센터를 사용하고 있다는 네이버는 화재 이후 얼마 지나지 않아 서비스가 복구 됐지만 카카오는 장장 8시간이 지나서야 일부 서비스를 겨우 복구 시켰을 뿐이고 완전 복구까지는 하루가 넘게 걸렸다.

그리고 오늘, 카카오는 지옥같은 월요일을 맛보고 있다. 개장과 함께 카카오 주식은 10% 가까이 추락했고, 몸값 비싼 개발자들을 데려다가 서비스 안전성 관리는 어떻게 하고 있냐는 비아냥에, 카카오 그룹 전 의장인 김범수가 국정감사에 출석요구를 받게되질 않나 경쟁사들은 이때다 싶어 자사 메신저를 광고하고 있다.

우리 프로젝트에서도 메신저 서버 안전성 관련 토론장이 열렸다.

## 22/10/19

- [액체괴물 ASMR #38 - 편안한 슬라임 영상 모음](https://youtu.be/wYZBKu1C6yI)

대표님이 게임 사운드 이펙트 디렉션 하시는 중 참고자료로 링크 걸어둔 영상이다.

게임 디렉션 하는 것을 보다 보면 조금 무섭기도 하다. 백엔드 로직, 보상 심리, 비쥬얼 적인 텍스쳐, 사운드의 질감 어디까지 봐야 되는 걸까. 너무 방대해서 자기 생활을 잃어버리는 건 아닐지 걱정이 들 정도다.


## 22/10/21

레이턴시 테스트 중 발생한 문제들. 네트워크 환경은 서버에서 중요하게 다뤄야 할 변수다. 최근에서야 레이턴스 테스트를 해보면서 클라이언트 측에 부작용이 발생하는 것을 파악했는데, 레이턴시 테스트는 좀 더 이르게 확인했어야 했나 싶다.

그렇지만 테스트를 한다고 할 때 자동화는 어렵다. 런칭 전 개발 시기에는 게임 플레이 시퀀스가 자꾸 변해서 테스트 범위을 정의하기가 어렵고 서버 프로토콜이나 클라 측 인터페이스가 언제 어떻게 변할지 예측할 수 없다. 음... 이제서야 레이턴시 취약점이 발견된 것을 너무 문제 삼는 것은 너무 과한 지적일까?

개발 과정에서 중간중간 주기적으로 레이턴시 안정성 체크해 줬어도 좋았을 것이다. 어려울 것 없다. 이번 네트워크 레이턴시 테스트에서는 요 툴을 사용했다. [Clumsy](https://jagt.github.io/clumsy/)

## 22/10/26

메이플 스토리 같은 경우는 이벤트 기간에 DB 응답 시간이 1시간 까지 지연되는 케이스가 있다고 한다. 인프라 조직에서 DB 응답성은 굉장히 예민한 주제라서 우리 서버 팀에게도 이런 저런 요구사항이 밀려오는 모양이다. 트래픽이 몰리는 상황에서도 최대한 빨리 대응할 수 있어야 한다. 캐시 효율을 최대한 높여야 할 것이다.

- 카카오에서는 모든 DB 마다 캐시를 달아놓는다고 한다. 적어도 우리 서버에서는 계정 접속 관련한 DB 요청에는 이런 처리가 들어가야 하지 않을까... 아니다. 첫 로그인 시점에서는 계정 정보를 캐싱한다는 게 어려울 수 있겠구나.

- 지스타에 작품 출품할 때에는 별의 별 상황이 다 있다고 한다. 보안문제가 대표적이다. 시연 기기에서는 와이파이를 차단해버리고서는 시연 기기들은 폐쇄망에 배포된 서버에 접속하게 하거나, 심지어는 기기마다 자체적으로 로컬 서버를 돌리고서는 룩백 ip로 서버 접속했다고 한다...

## 22/10/27

가끔 이름을 고민할 땐 대상의 성질이 아니라, 대상을 대하는 주체의 태도를 고려하는 것이 좋을 수 있다.