# AtDevcat_2024

## 24.02.27

[c - Is it Safe to use ReaderWriterLockSlim in an async method - Stack Overflow](https://stackoverflow.com/questions/15882710/is-it-safe-to-use-readerwriterlockslim-in-an-async-method)

```cs
System.Threading.ReaderWriterLockSlim readerwriterlock = new System.Threading.ReaderWriterLockSlim();
private async Task Test()
{
    readerwriterlock.EnterWriteLock();
    await Task.Yield(); //do work that could yield the task
    readerwriterlock.ExitWriteLock(); //potentailly exit the lock on a different thread
}
```

ìœ„ì˜ ì½”ë“œëŠ” ì•ˆì „í•˜ì§€ ëª»í•˜ë‹¤.

`await Task.Yield();` ë¼ì¸ ì „í›„ë¡œ ì‹¤í–‰ ìŠ¤ë ˆë“œê°€ ë‹¤ë¥¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤!!!

ë½ì„ ê±°ëŠ” ìŠ¤ë ˆë“œì™€ í•´ì œí•˜ëŠ” ìŠ¤ë ˆë“œê°€ ë‹¤ë¥´ë©´ ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì´ ìœ„ë°˜ëœë‹¤. ì •ë§ ì •ë§ í°ì¼ ë‚  ìˆ˜ ìˆë‹¤!!!

**ChatGPT ë‹µë³€**

`ReaderWriterLockSlim`ì„ ë¹„ë™ê¸° ì½”ë“œ ë‚´ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì£¼ì˜ë¥¼ ìš”í•©ë‹ˆë‹¤. ìœ„ì˜ ì½”ë“œ ì˜ˆì œì—ì„œëŠ” ë¹„ë™ê¸° ë©”ì„œë“œ(Loop1 ë° Loop2) ë‚´ì—ì„œ ReaderWriterLockSlimì˜ EnterWriteLockê³¼ ExitWriteLockì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ReaderWriterLockSlimì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì¼ë°˜ì ìœ¼ë¡œ ì•ˆì „í•´ ë³´ì´ì§€ë§Œ, ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì˜ ë§¥ë½ì—ì„œ ëª‡ ê°€ì§€ ì£¼ì˜í•´ì•¼ í•  ì ì´ ìˆìŠµë‹ˆë‹¤:

ë¹„ë™ê¸°ì™€ ë¸”ë¡œí‚¹ í˜¸ì¶œì˜ í˜¼í•© ì‚¬ìš©: ReaderWriterLockSlimì˜ EnterWriteLock ë©”ì„œë“œëŠ” í˜¸ì¶œ ìŠ¤ë ˆë“œë¥¼ ë¸”ë¡œí‚¹í•©ë‹ˆë‹¤. ì´ëŠ” await Task.Delay(1000);ì™€ ê°™ì€ ë¹„ë™ê¸° í˜¸ì¶œ í›„ì— ì´ë£¨ì–´ì§ˆ ê²½ìš°, ë¹„ë™ê¸° ì‘ì—…ì´ ì™„ë£Œëœ í›„ ë™ê¸° ë¸”ë¡œí‚¹ í˜¸ì¶œì´ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ íŒ¨í„´ì€ ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥ë˜ì§€ ì•Šìœ¼ë©°, íŠ¹íˆ UI ìŠ¤ë ˆë“œì™€ ê°™ì´ ë¸”ë¡œí‚¹ë˜ì–´ì„œëŠ” ì•ˆ ë˜ëŠ” ìŠ¤ë ˆë“œì—ì„œëŠ” ì‚¬ìš©ì„ í”¼í•´ì•¼ í•©ë‹ˆë‹¤.

ë°ë“œë½ ìœ„í—˜: ìœ„ì˜ ì˜ˆì‹œ ì½”ë“œ ìì²´ì—ì„œ ì§ì ‘ì ì¸ ë°ë“œë½ì„ ìœ ë°œí•  ëª…í™•í•œ ìš”ì†ŒëŠ” ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ReaderWriterLockSlimì„ ì‚¬ìš©í•˜ëŠ” ë³µì¡í•œ ë¹„ë™ê¸° íë¦„ì—ì„œëŠ” ë°ë“œë½ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•˜ë‚˜ì˜ íƒœìŠ¤í¬ê°€ ì“°ê¸° ë½ì„ íšë“í•œ ìƒíƒœì—ì„œ ë‹¤ë¥¸ íƒœìŠ¤í¬ê°€ ê°™ì€ ë½ì„ ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ, ì²« ë²ˆì§¸ íƒœìŠ¤í¬ê°€ í›„ìì˜ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ê²Œ ë˜ë©´ ë°ë“œë½ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë¹„ë™ê¸° ì½”ë“œì—ì„œì˜ ë½ ê´€ë¦¬: ReaderWriterLockSlimì€ ë³¸ì§ˆì ìœ¼ë¡œ ë™ê¸°ì  ë½ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ë¹„ë™ê¸° ì½”ë“œì—ì„œëŠ” SemaphoreSlimì˜ WaitAsync ë©”ì„œë“œì™€ ê°™ì´, ë¹„ë™ê¸°ì ìœ¼ë¡œ ëŒ€ê¸°í•  ìˆ˜ ìˆëŠ” ë½ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë©”ì»¤ë‹ˆì¦˜ì€ ë¹„ë™ê¸° ì‘ì—…ì´ ë½ì„ ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ ìŠ¤ë ˆë“œë¥¼ ë¸”ë¡œí‚¹í•˜ì§€ ì•Šê³  ëŒ€ê¸°í•˜ë„ë¡ í•´ì¤ë‹ˆë‹¤.

ìš”ì•½í•˜ìë©´, ReaderWriterLockSlimì€ ì£¼ë¡œ ë™ê¸° ì½”ë“œì—ì„œ ì‚¬ìš©ë  ë•Œ ì•ˆì „í•˜ë©°, ë¹„ë™ê¸° ì½”ë“œì—ì„œ ì‚¬ìš©í•  ë•ŒëŠ” ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. íŠ¹íˆ, ë¹„ë™ê¸° ì‘ì—… ë‚´ì—ì„œ ë¸”ë¡œí‚¹ ë½ í˜¸ì¶œì„ í”¼í•˜ê³ , ê°€ëŠ¥í•˜ë‹¤ë©´ ë¹„ë™ê¸° ì‘ì—…ì— ì í•©í•œ ë½ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë¹„ë™ê¸° ì½”ë“œì—ì„œ ReaderWriterLockSlim ê°™ì€ ë™ê¸° ë½ì„ ì‚¬ìš©í•  ë•ŒëŠ” ë°ë“œë½ì´ë‚˜ ì„±ëŠ¥ ì €í•˜ì˜ ìœ„í—˜ì„ ì¶©ë¶„íˆ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

## 24.02.28

[White House - PRESS RELEASE: Future Software Should Be Memory Safe](https://www.whitehouse.gov/oncd/briefing-room/2024/02/26/press-release-technical-report/)

> ONCD makes the case that technology manufacturers can prevent entire classes of vulnerabilities from entering the digital ecosystem by adopting memory safe programming languages. ONCD is also encouraging the research community to address the problem of software measurability to enable the development of better diagnostics that measure cybersecurity quality.

ë°±ì•…ê´€ì—ì„œ ë””ì§€í„¸ í™˜ê²½ì—ì„œ ì‚¬ì´ë²„ ê³µê²©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ë©”ëª¨ë¦¬ ì„¸ì´í”„í•œ ì†Œí”„íŠ¸ì›¨ì–´ì˜ í•„ìš”ì„±ì„ ê°•ì¡°í•¨.

**ê²Œì„ ê¸°ë™ ì†ë„ ë†’ì´ê¸°**

- í˜„ëŒ€ì¸ì€ ì¸ë‚´ì‹¬ì´ ë§¤ìš° ë¶€ì¡±í•˜ê³  ì§§ì€ ì‹œê°„ì—ë„ ì§‘ì¤‘ë ¥ì„ ìƒëŠ”ë‹¤.
  - ê²Œì„ ì´ˆê¸° ê¸°ë™ ì‹œê°„ì´ ê¸¸ë©´ ìœ ì € ì´íƒˆë¡œ ì´ì–´ì§
- ê¸°ë™ ì†ë„ë¥¼ ì¤„ì´ê±°ë‚˜, ëŒ€ê¸° ì¤‘ì— ì£¼ì˜ë¥¼ í™˜ê¸°ì‹œí‚¬ë§Œí•œ ë‹¤ë¥¸ ìš”ì†Œë¥¼ ì§‘ì–´ë„£ì–´ì•¼ í•œë‹¤. (ë™ì˜ìƒì„ íŠ¼ë‹¤ê±°ë‚˜)

[Jake Seo on X ì„¸ê³„ ìµœê³ ì˜ ê°œë°œì ì¤‘ í•œëª…ì¸ ì¡´ ì¹´ë§¥ì´ ì‹œê°„ì„ ê´€ë¦¬í•˜ëŠ” ë°©ì‹ ì¸í„°ë·°í•œ ê²ƒì„ ë‚˜ë¦„ëŒ€ë¡œ ì •ë¦¬í•´ë´„ ì¸í„°ë·° ì˜ìƒì´ ë§ˆì¹˜ ì»¤í”¼ì±—í•˜ëŠ” ê²ƒ ê°™ì•„ ì¬ë°Œì—ˆë‹¤ - ëª‡ì‹­ë…„ê°„ ì£¼ 60ì‹œê°„ ì´ìƒ ì¼í•´ì˜´ - ë§¤ì¼ 10ì‹œê°„ ì •ë„ - ì Šì—ˆì„ ë•ŒëŠ” êµ‰ì¥íˆ ë¶ˆê·œì¹™ì ìœ¼ë¡œ ì‚´ì•˜ìŒ - ë°¤ì„ ìƒˆê±°ë‚˜ ëŠ¦ì ì„ ìê¸°ë„ í•¨ - ë‚˜ì´ê°€ ë“¤ì–´ì„œâ€¦  X](https://twitter.com/JakeSeo8/status/1733483508549693512?s=20)

ì„¸ê³„ ìµœê³ ì˜ ê°œë°œì ì¤‘ í•œëª…ì¸ ì¡´ ì¹´ë§¥ì´ ì‹œê°„ì„ ê´€ë¦¬í•˜ëŠ” ë°©ì‹ ì¸í„°ë·°í•œ ê²ƒì„ ë‚˜ë¦„ëŒ€ë¡œ ì •ë¦¬í•´ë´„

ì¸í„°ë·° ì˜ìƒì´ ë§ˆì¹˜ ì»¤í”¼ì±—í•˜ëŠ” ê²ƒ ê°™ì•„ ì¬ë°Œì—ˆë‹¤

- ëª‡ì‹­ë…„ê°„ ì£¼ 60ì‹œê°„ ì´ìƒ ì¼í•´ì˜´
- ë§¤ì¼ 10ì‹œê°„ ì •ë„
- ì Šì—ˆì„ ë•ŒëŠ” êµ‰ì¥íˆ ë¶ˆê·œì¹™ì ìœ¼ë¡œ ì‚´ì•˜ìŒ
- ë°¤ì„ ìƒˆê±°ë‚˜ ëŠ¦ì ì„ ìê¸°ë„ í•¨
- ë‚˜ì´ê°€ ë“¤ì–´ì„œ ì•„ì´ë¥¼ ë‚³ê³  ë‚˜ì„œëŠ” ì¼ë°˜ ì§ì¥ì¸ê³¼ ë¹„ìŠ·í•œ íŒ¨í„´ìœ¼ë¡œ ì‚¶
- ìµœê·¼ ëª‡ë‹¬ê°„ 4ë§ˆì¼ (ì•½ 6km) ì •ë„ë¥¼ ê±·ê±°ë‚˜ ë›°ëŠ”ë° íš¨ê³¼ê°€ ìˆëŠ” ê²ƒ ê°™ë‹¤
- ìš´ë™í•œ ë‹¤ìŒ ìƒ¤ì›Œ í›„ì˜ ëª‡ì‹œê°„ì´ ìì‹ ì—ê²Œ ìµœê³ ì˜ ì‹œê°„ì´ì—ˆë‹¤
- ë‚œ 12ì‹œê°„ë§Œ ì¼í•˜ë©´ ë‡Œê°€ ì§€ì¹˜ëŠ”ë° 20ì‹œê°„ì”© ì¼í•˜ëŠ” í”„ë¡œê·¸ë˜ë¨¸ê°€ ë¶€ëŸ½ë‹¤
- 4ì‹œê°„ë§Œ ìê³  ì¼í•˜ëŠ” ë…ì¢…ì„ ì´ê¸°ê¸´ ì–´ë µë‹¤
- ì‚¬ëŒë“¤ì—ê²ŒëŠ” 8ì‹œê°„ì€ ê¼­ ìë¼ê³  ì¡°ì–¸í•œë‹¤
- ìŠ¤ìŠ¤ë¡œë¥¼ ì••ë°•í•˜ì§€ ë§ê³  ì–µì§€ë¡œ ì¼ì° ì¼ì–´ë‚˜ì§€ ë§ë¼ê³ 
- í•œ ê²Œì„ ê°œë°œìì˜ ë§ì„ ë¹Œë¦¬ë©´ "ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ì¼ì€ ë°˜ë“œì‹œ í° ë¬¸ì œë¥¼ ê°€ì ¸ì˜¨ë‹¤" ë¼ê³  í•¨
- ëª‡ëª‡ ì‚¬ëŒë“¤ì€ ìƒì‚°ì„±ì— ëŒ€í•œ ì§€ì‹ì„ ì˜ëª» ì¸ìš©í•˜ë©° ì£¼ 40ì‹œê°„ ì¼í•˜ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë¼ê³  ë§í•œë‹¤
- ë„ˆë¬´ ì˜¤ë˜ ì¼í•˜ë©´ ìƒì‚°ì„±ì´ ê°ì†Œí•˜ê¸´ í•˜ì§€ë§Œ ëª‡ì‹œê°„ ë” ì¼í•œë‹¤ê³  ì„±ê³¼ê°€ ë‚˜ë¹ ì§€ì§„ ì•ŠìŒ
- ë‹¹ì¥ ì§€êµ¬ì— ì†Œí–‰ì„±ì´ ë–¨ì–´ì§„ë‹¤ë©´, ì—¬ëŸ¬ë¶„ì€ ì¼ë¡ ë¨¸ìŠ¤í¬ê°€ ì´ë¥¼ ë§‰ê¸° ìœ„í•´ ì¼í•˜ë©´ì„œ ì˜¤í›„ 5ì‹œì— ì¹¼í‡´í•˜ëŠ” ê²ƒì„ ë‘ê³ ë³¼ ê²ƒì¸ê°€?
- ë‚˜ë¥¼ ì¬ìˆ˜ ì—†ëŠ” ë†ˆ (Asshole) ìœ¼ë¡œ ë³´ëŠ” ì‹œì„ ë„ ìˆê² ì§€ë§Œ, ì§„ì‹¤ì€ ë” ì˜¤ë˜ ì¼í• ìˆ˜ë¡ ë” ë§ì€ ì„±ê³¼ë¥¼ ì–»ëŠ”ë‹¤
- ëª¨ë‘ê°€ ì—´ì‹¬íˆ ì¼ í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ ë¬´ì–¸ê°€ë¥¼ ì´ë£¨ë ¤ë©´ ì¼ì— ë§¤ì§„í•´ì•¼ í•œë‹¤
- ë‚œ ì¼ë°˜ì ì¸ ì§ì¥ì¸ê³¼ ë§¤ìš° ë‹¤ë¥´ë‹¤
- ì¼ì´ ë‚´ ì‚¶ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•œ ì¸ì‹ë¶€í„° ë§¤ìš° ë‹¤ë¥´ë‹¤
- ì¼ì„ í•˜ë©° ë²ˆì•„ì›ƒì„ ê²ªì€ ì ì´ í•œë²ˆë„ ì—†ë‹¤
- ê·¸ëŸ¬ë‚˜ ì£¼ë³€ì—ëŠ” ìœ„ëŒ€í•œ ì„±ê³¼ë¥¼ ê±°ë‘ê³  ì™„ì „íˆ ì§€ì¹œ ì‚¬ëŒë„ ë§ë‹¤
- ì§€ë£¨í•˜ë‹¤ê³  ëŠê»´ì§„ë‹¤ë©´ ë‹¤ë¥¸ ì—…ë¬´ì— ì „ë…í•˜ë©° ë‹¤ì‹œ í˜ì„ ì–»ì—ˆë‹¤
- ì´ë¥¸ë°” ì¢‹ì€ ëŠ¥ë ¥ì´ë€ ìŒ“ì—¬ìˆëŠ” ìˆ˜ë§ì€ ê³¼ì œ ì¤‘ì—ì„œ ê°€ì¥ ì˜ ë§ëŠ” ê²ƒì„ ì„ íƒí•´ íš¨ìœ¨ì ìœ¼ë¡œ ì§‘ì¤‘í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤
- ë…¼ë¬¸ì´ë‚˜ ë¦¬í¬íŠ¸ë¥¼ ì½ë˜, ê³¼ê±° ìë£Œë¥¼ ì •ë¦¬í•˜ë˜ ì½”ë“œë¥¼ ì§œë˜ ë¶„ëª… ê·¸ ê³¼ì œë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•´ë‚¼ ìˆ˜ ìˆëŠ” ì‹œê°„ëŒ€ê°€ ì¡´ì¬í•œë‹¤
- í•  ì¼ì„ ì ì ˆíˆ ë°”ê¾¸ëŠ” ê²ƒì€ ì •ë§ ê°€ì¹˜ìˆëŠ” ëŠ¥ë ¥ì´ë‹¤
- ì‚¬ëŒë“¤ì€ ë‚˜ë¥¼ ë¬´í‘œì •í•˜ê²Œ ì¼í•˜ê³  ë¬¸ì„œë§Œ ì³ë‹¤ë³´ëŠ” ìŒì¹¨í•œ í”„ë¡œê·¸ë˜ë¨¸ë¡œ ë¬˜ì‚¬í•œë‹¤
- ë‚œ í•œë²ˆ ì¼í•˜ë©´ ì§‘ì¤‘í•´ì„œ ì„±ê³¼ë¥¼ ë‚´ê¸° ë•Œë¬¸ì— ì–´ëŠì •ë„ ë§ëŠ” ì–˜ê¸°ì§€ë§Œ ë‚´ ì¼í•˜ëŠ” ë°©ì‹ì€ ëŒ€ë¶€ë¶„ì—ê² ë§ì§€ ì•ŠëŠ”ë‹¤
- ê·¸ëŸ¬ë‚˜ ì—´ì‹¬íˆ ì¼í•˜ëŠ” ê²ƒì´ ì–´ë¦¬ì„ì€ ê±´ ì ˆëŒ€ ì•„ë‹ˆë‹¤
- ì¼ì— ëŒ€í•œ ì§‘ì°© ì†ì—ì„œ ì‚¶ê³¼ ì¦ê±°ì›€ì„ ì°¾ëŠ” ì‚¬ëŒë„ ì¡´ì¬í•˜ë©° ê·¸ë“¤ì€ ì¼ë°˜ì ì¸ ì‚¬ëŒë“¤ë³´ë‹¤ ë” ë§ì€ ì„±ê³¼ë¥¼ ë‚¼ ê²ƒì´ë‹¤
- ì‚¬ê³  ì‹¶ì€ ë§Œí™” ì±…ê³¼ ìŒì‹ì„ ë§˜ê» ì‚¬ëŠ” ê²Œ ë‚´ ì¸ìƒì˜ ë‚™ì´ë‹¤.
- ë§ì€ ëˆì´ ë“¤ì§„ ì•Šì§€ë§Œ ë‚´ í™œë ¥ì˜ ì›ì²œì´ë‹¤
- ë§¨ë‚  ê°™ì€ í”¼ìë§Œ ì‹œì¼°ëŠ”ë° ì•Œê³ ë³´ë‹ˆ ê·¸ í”¼ì ê°€ê²Œê°€ ë‚˜ì—ê²Œë§Œ ì—„ì²­ë‚œ í• ì¸ì„ í•´ì£¼ì—ˆë‹¤
- ìš´ë™ì„ ê¾¸ì¤€íˆ í•˜ê³  ì ˆëŒ€ ê³¼ì‹í•˜ì§€ ì•Šê¸°ì— ê±´ê°•ì´ ê·¸ë ‡ê²Œ ë‚˜ì˜ì§„ ì•Šë‹¤
- ì§€ê¸ˆ ë‚˜ì˜ ì†Œìš¸í‘¸ë“œëŠ” ì œë¡œì½œë¼ì´ë‹¤ í•˜ë£¨ì— 8~9 ìº”ì€ ë§ˆì‹œëŠ” ê²ƒ ê°™ë‹¤

**ìˆ™ë ¨ëœ ì‚¬ëŒë“¤ì´ ë¹ ì§€ê¸° ì‰¬ìš´ í•¨ì •**

- ì¼ì˜ ëª©ì ë³´ë‹¤ ìì‹ ì´ ê°€ì§€ê³  ìˆëŠ” ì „ë¬¸ ëŠ¥ë ¥ì„ ë°œíœ˜í•¨ì—ì„œ ë§Œì¡±ì„ ì–»ì„ ë•Œê°€ ë§ìŒ
- ì „ë¬¸ ê¸°ìˆ ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ëª©ì ì— ì§‘ì¤‘í•˜ì§€ ì•Šê³  ê³¼ì •ì— ë§Œì¡±í•˜ëŠ” í•¨ì •ì— ë¹ ì§€ê¸° ì‰½ë‹¤.
- ë¹„ìŠ·í•œ ì¼ì„ ë°˜ë³µí•´ì„œ ìˆ˜í–‰í•˜ë‹¤ë³´ë©´
  - ëª©ì ì„ ìŠê³  ë°˜ë³µì ì¸ í–‰ë™ ê·¸ ìì²´ê°€ ëª©ì ì¸ ê²ƒì„ ì°©ê°í•˜ëŠ” ì¼ì´ ë§ë‹¤.

## 24.03.08

[Large Language Models Are Drunk at the Wheel  Matt.si](https://matt.si/2024-02/llms-overpromised/)

- LLM ê¸°ë°˜ ai ê´‘í’ì„ ì§€ì í•˜ëŠ” ê¸€
- êµ‰ì¥íˆ ë„“ì€ ë¶„ì•¼ì˜ 100% ê²€ì¦ë˜ì§€ ëª»í•œ ë¹…ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•œ LLMì€ ì–¸ì œ ì–´ë–¤ ì˜¤ë‹µì„ ë‚´ë†“ì„ ì§€ ëª¨ë¥´ëŠ” í˜¼ë€ìŠ¤ëŸ¬ìš´ ìƒíƒœì„
  - í˜„ì¬ ai ë²„ë¸”ì— í˜„í˜¹ë˜ì§€ ë§ ê²ƒì„ ì£¼ì¥
- í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ì œí•œëœ ë²”ìœ„ì˜ ì „ë¬¸ ë¶„ì•¼ë¥¼ í•™ìŠµí•œ AI ë¼ë©´ ì‹ ë¢°í•  ìˆ˜ ìˆë‹¤.

For an easier-to-read "layman" explanation I recommend Spencer Torene's (Computational Neuroscience PhD, Lead Research Scientist at Reuters) October article "Do LLMs Reason?"39. In short, LLMs are like parrots. However, their behaviour frequently appears logical. This is because their training sets are so vast and the compute power dedicated to their training so enormous that they are often capable of retrieving (or parroting) a believable answer. But they do not perform the logical steps to actually solve the problem. As such they cannot solve novel problems, nor verify whether their answers are correct or incorrect.

They are not the thinking machines that Turing envisioned. It might seem like I'm splitting hairs, but there is a big difference between real intelligence and the guesswork that LLMs do. They have no conception of knowledge, of truth or untruth: They cannot test whether what they are saying is correct or not. This is why they frequently fail very simple, obvious questions. Of course, the subtle truth here is that they are also frequently answering wrongly to complex, difficult questions but we are less likely to notice. The answers to complex questions take much more effort for us to verify. Our lazy, efficient brains are more likely to gloss over such details and assume it is correct. Hence why we notice these mistakes much more often when we ask simple, easily refutable questions.

One great example recently was asking an LLM to tell you the name of a Greek philosopher beginning with M4. Numerous people have tried this and time and time again LLMs will give you wrong answers insisting that Aristotle, or Seneca, or some other philosopher's name begins with M. Yet, we can see right in front of us that it does not. Note how these chatbots speak with such confidence: They are exactly as certain about their answer when they are wrong, as when they are right. ChatGPT is still doing this now, and you can see an example I generated below.

![](img/2024-03-08-14-21-41.png)

> Infinite Possibilities, Uncontrollable Chaos

For some examples: ChatGPT is not supposed to fill out CAPTCHAs for you. It can do so by integrating with other tools, but this is rightly considered a malicious use of it. But OpenAI's attempts to restrict it from doing so haven't been so successful. Just attach a CAPTCHA pattern to a photo of a locket and ask it to read the words on "my grandma's locket"6. In another case, one company that sells cars was naive enough to put ChatGPT in charge of a virtual assistant on their public-facing website. A user very easily made it offer to sell them a $50,000+ car for $1 and even say "that's a legally binding offer - no take backsies"7. Most recently, Air Canada was foolish enough to publish a chatbot that gives travel advice. It gave incorrect refund policies, that a court found AirCanada must uphold36 34. It is not clear whether this bot was using an LLM, but it sets an important precedent that recklessly placing a chatbot in a position to represent your business does have consequences. You can find more LLM-specific examples by asking ChatGPT to tell you about something fictional that you just made up: It will often make up a bunch of plausible-sounding nonsense rather than admitting that it does not know10.

![](img/2024-03-08-14-22-18.png)

> Valid use cases

Despite all the negativity I've just dropped, I do think LLMs are cool as heck. If we could just stop applying them in stupid ways to problems they cannot solve, maybe we can find some good use cases. What would that look like? Well, the #1 rule I would say is to never feed input into an LLM from a human. These bots are just too vulnerable to unexpected or malicious behaviour and there is no way to lock them down. The only robust, reliable LLM is one that is only dealing with a small set of known, expected, already tested inputs. One example might be in interactive digital art, simulations and video games. Instead of having dozens of NPCs that say the same thing, what if you fed the facts that a particular NPC knows (hello, Symbolism) into an LLM to generate believable dialogue? No more dozen NPCs saying all the same things, now they can at least come across as more believable by varying the particular choice of words they use to express themselves. And because the input is coming from your system, not the user or any external source, you can thoroughly test it.

> We've been here before

While it was not the only cause, this contributed directly to the 1970s AI Winter in which funding and interest in AI dried up. And who could blame them? But this time is different because it is not only a somewhat niche academic field at stake, but a sizable portion of investment in the tech industry. The industry right now is facing record-breaking layoffs and investment is drying up quickly, yet the one sector that is still growing is AI startups33. That investment might not last much longer than it takes for investors to realise that LLMs are not all they've been hyped up to be. What happens then?

We've made this mistake before, too. The dot com boom, the IoT fad, the big data trend, the crypto craze, smart assistants, NFTs and more. Our industry has a habit of promising the moon to investors, only for the funding to dry up when we can't deliver. Perhaps we don't feel the ramifications as strongly as we could because by the time the last bubble bursts, we are already hyping up investors for the next big fad. I recall during the big data craze a startup that had no production systems or customers, yet hired a data scientist. After a few weeks, he quietly confessed to me that he had nothing to do. There was no data with which to perform data science because they had no customers. Yet when the investors came to the office, one of the first places the executives would take them to was to see the data scientist so they could proudly show off that they were following the latest tech trend. Even if it made no sense in the context of their business. The same company produced multiple smart assistants when that was the cool thing to do, for a product that didn't have any customers for the growing collection of smart assistants to talk to.

> Conclusion

If we ever develop true artificial intelligence, it will be as similar to LLMs as a jet airliner is to a paper aeroplane. When someone knocks on your door promising to sell you a bridge with LLMs, look at them very sceptically. And then close the door in their face. Most of the organisations I've seen investing in LLM fantasies have one thing in common: Their product has tons of problems that their time would be better spent addressing. Think seriously about the design of your software, its reliability, and its usability. Spend your resources, your time and attention where it is most needed.

Whether you are a developer, designer, product manager, or anyone involved in creating software: In all your dealings with LLMs please reflect soberly on your professionalism and your responsibility to your users and stakeholders. If we aspire to the same earnest commitment and integrity that is seen in engineering, architecture, law, and so forth then let us act like it. I will leave you with this quote from the Engineers Council for Professional Development's definition of engineering38 37:

[Lessons Learned in Software Development  Henrik Warne's blog](https://henrikwarne.com/2015/04/16/lessons-learned-in-software-development/)

- (3)Add logging and error handling early.
  - When developing a new system, one of the first things I do is adding logging and error handling, because both are useful from the very beginning. For all systems that are bigger than a handful of lines of code, you need some way of knowing what happens in the program. Perhaps not when it is working as expected, but as soon as it doesnâ€™t, you must be able to see whatâ€™s happening. The same goes for error handling â€“ errors and exceptions happen in the beginning too, so the sooner you handle them in a systematic way, the better.
- (16)Rubber ducking.
  - Whenever you are stuck, go to a colleague and explain the problem to them. Many times, as you talk, you realize what the problem is, even if your colleague doesnâ€™t say a word. Sounds like magic, but works surprisingly often.
- (20)Sleep on it.
  - If you are working on a difficult problem, try to get in a nightâ€™s sleep before you decide. Then your subconscious mind works on the problem even when you arenâ€™t actively thinking about it. As a result, the solution can seem obvious the next day.

## 24.03.12

**C# Stopwatchì˜ í‹±ê³¼ DateTimeì˜ í‹±ì€ ë‹¤ë¦„**

[MSDN .NET 7 / DateTime.Ticks ì†ì„±](https://learn.microsoft.com/ko-kr/dotnet/api/system.datetime.ticks?view=net-7.0)

- **A single tick represents one hundred nanoseconds or one ten-millionth of a second**. There are 10,000 ticks in a millisecond (see TicksPerMillisecond) and 10 million ticks in a second.
  - 1 DateTime.Ticks = 100 ns(ë‚˜ë…¸ì´ˆ)

[MSDN .NET 7 / Stopwatch.ElapsedTicks ì†ì„±](https://learn.microsoft.com/en-us/dotnet/api/system.diagnostics.stopwatch.elapsedticks?view=net-7.0)

- **This property represents the number of elapsed ticks in the underlying timer mechanism. A tick is the smallest unit of time that the Stopwatch timer can measure**. Use the Frequency field to convert the ElapsedTicks value into a number of seconds.
- Stopwatch ticks are different from DateTime.Ticks. Each tick in the DateTime.Ticks value represents one 100-nanosecond interval. Each tick in the ElapsedTicks value represents the time interval equal to 1 second divided by the Frequency.
  - 1 Stopwatch.ElapsedTicksëŠ” í•˜ë“œì›¨ì–´ í´ë½ì— ê¸°ë°˜í•´ì„œ ì¸¡ì •
  - Stopwatch.ElapsedTicks = 1 second / Stopwatch.Frequency

## 24.03.13

[í˜ì´ìŠ¤ë¶ì´ Gitì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ìœ   GeekNews](https://news.hada.io/topic?id=13769)

- Mercurial ì„ ì‚¬ìš©í•œë‹¤ê³  í•¨
- [Mercurial SCM](https://www.mercurial-scm.org/)
- [Scaling Mercurial at Facebook - Engineering at Meta](https://engineering.fb.com/2014/01/07/core-infra/scaling-mercurial-at-facebook/)

===

**Graphite ê°œë°œì Greg Fosterê°€ ì´ì— ê´€ì‹¬ì„ ê°€ì§„ ì´ìœ **

- ì™œ í˜ì´ìŠ¤ë¶ì€ Git ëŒ€ì‹  Mercurialì„ ì±„íƒí•˜ê³  ê·¸ ìœ„ì— ì»¤ìŠ¤í…€ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í–ˆì„ê¹Œ ?
- êµ¬ê¸€ë„ Gitì„ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, ê·¸ê±´ Googleì˜ ì—”ì§€ë‹ˆì–´ë§ì´ Gitë³´ë‹¤ 5ë…„ ì´ìƒ ì•ì„œ ìˆê¸° ë•Œë¬¸
- ë°˜ë©´ì— Facebookì€ Gitì´ ë§Œë“¤ì–´ì§„ ì‹œê¸°ì™€ ë¹„ìŠ·í•œ ì‹œê¸°ì¸ 2004ë…„ì— ì„¤ë¦½ë˜ì—ˆìœ¼ë©°, Facebookì´ ì†ŒìŠ¤ ì œì–´ ë„êµ¬ë¥¼ ì§„ì§€í•˜ê²Œ ê²€í† í•  ë¬´ë µì—ëŠ” Mercurialë³´ë‹¤ Gitì´ ë” ì¸ê¸°ê°€ ìˆì—ˆìŒ
- ê·¸ëŸ°ë° ì™œ Facebookì€ Gitì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ê¹Œ?
  - ê·¸ì˜ ìƒê°ì— Facebookì´ Gitì„ ë„ì…í•˜ê³  2010ë…„ëŒ€ ì´ˆë°˜ì— ê¸°ì—¬í–ˆë‹¤ë©´ ì—”ì§€ë‹ˆì–´ë§ ì„¸ê³„ëŠ” ë‹¬ë¼ì¡Œì„ ê²ƒ
  - Gitì´ ë” ì‚¬ìš©ì ì¹œí™”ì ì´ê³ , ë„¤ì´í‹°ë¸Œí•˜ê²Œ Stacked Changesë¥¼ ì§€ì›í•˜ê²Œ ë˜ì—ˆì„ ìˆ˜ë„
- ì´ˆê¸° í˜ì´ìŠ¤ë¶ ì§ì›ë“¤ì´ ë§Œë“  Uberì™€ Pinterest ê°™ì€ íšŒì‚¬ë“¤ë„ Mercurial ëŒ€ì‹  Gitê³¼ GitHubë¥¼ ì†ŒìŠ¤ ì»¨íŠ¸ë¡¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì§€ë‚œ 10ë…„ ë™ì•ˆ ëœ íŒŒí¸í™”ëœ ìƒíƒœê³„ë¥¼ ë§Œë“¤ì—ˆì„ ê²ƒ
- í•˜ì§€ë§Œ Facebookì€ (ê¸°ë³¸ ëª¨ë…¸ë¦¬í¬ì§€í† ë¦¬ë¥¼ ìœ„í•´) Gitì„ ê³ ìˆ˜í•˜ì§€ ì•Šì•˜ìŒ. ëŒ€ì‹  ë²„ì „ ê´€ë¦¬ë¥¼ ìœ„í•´ Mercurialì„ ì±„íƒí•˜ê³  ê·¸ ìœ„ì— ì‚¬ìš©ì ì§€ì • ë„êµ¬ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€í•¨
  - í˜ì´ìŠ¤ë¶ì— ì˜¬ë¼ì˜¨ Scaling Mercurial at Facebook ê¸€ì„ ë°œê²¬
  - 10ë…„ì „ ê¸€ì´ê³ , ê·¸ ì´í›„ì— ëª‡ê°œì˜ ìœ íŠœë¸Œë¥¼ í†µí•´ì„œ "ì„±ëŠ¥ ë•Œë¬¸ì´ì§€" ë¼ëŠ” í•´ë‹µì„ ì–»ì—ˆìŒ
  - í•˜ì§€ë§Œ, ë” ê¹Šê²Œ ë“¤ì–´ê°€ì„œ ê·¸ ë‹¹ì‹œ ê²°ì •ê¶Œìë“¤ì˜ ìƒê°ì„ ë“¤ì–´ë³´ê³  ì‹¶ì—ˆê³ , Mercurial ë§ˆì´ê·¸ë ˆì´ì…˜ í”„ë¡œì íŠ¸ì— ì°¸ì—¬í–ˆë˜ ë‘ëª…ì˜ ì—”ì§€ë‹ˆì–´ì—ê²Œ ë¬¼ì–´ë´¤ìŒ
  - ê·¸ë“¤ê³¼ì˜ ë¹„ê³µì‹ì ì¸ ì–˜ê¸°ë¥¼ í†µí•´ ì´ ë‚´ìš©ì„ ì •ë¦¬í•¨

**í˜ì´ìŠ¤ë¶ì´ Gitì„ ë²„ë¦¬ê³  Mercurialë¡œ ì´ì£¼í•œ ì´ìœ **

- í˜ì´ìŠ¤ë¶ì€ ì²˜ìŒì— Gitì„ ì‚¬ìš©í–ˆìœ¼ë‚˜, 2012ë…„ê²½ ì½”ë“œë² ì´ìŠ¤ ê·œëª¨ê°€ ì»¤ì§€ë©´ì„œ ì„±ëŠ¥ ë¬¸ì œë¥¼ ê²ªê¸° ì‹œì‘
- Gitì˜ íŒŒì¼ "stat-ing" ê³¼ì •ì´ ë³‘ëª© í˜„ìƒì„ ì¼ìœ¼í‚¤ë©°, ê¸°ë³¸ Git ëª…ë ¹ì–´ ì‹¤í–‰ ì‹œê°„ì´ 45ë¶„ ì´ìƒ ì†Œìš”
- Git ìœ ì§€ë³´ìˆ˜ìë“¤ì€ í˜ì´ìŠ¤ë¶ì˜ ëŒ€ê·œëª¨ ë¦¬í¬ì§€í† ë¦¬ ë¬¸ì œì— ëŒ€í•´ í˜‘ë ¥ì ì´ì§€ ì•Šì•˜ê³ , ëŒ€ì‹  ë¦¬í¬ì§€í† ë¦¬ ë¶„í• ì„ ê¶Œì¥

**ê³ ë ¤ëœ ëŒ€ì•ˆë“¤**

- 2012ë…„ ë‹¹ì‹œ Gitì˜ ëŒ€ì•ˆì€ ë§ì§€ ì•Šì•˜ìœ¼ë©°, í˜ì´ìŠ¤ë¶ì€ Perforceì™€ ê°™ì€ íì‡„ ì†ŒìŠ¤ ì†”ë£¨ì…˜ì„ ê³ ë ¤í–ˆìœ¼ë‚˜ ê¸°ìˆ ì  ë¬¸ì œê°€ ìˆì—ˆìŒ
- Mercurialì€ Gitê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ê°€ì¡Œìœ¼ë‚˜, ë” ê¹”ë”í•œ ì•„í‚¤í…ì²˜ì™€ í™•ì¥ì„±ì„ ê°€ì§€ê³  ìˆì—ˆìŒ
- í˜ì´ìŠ¤ë¶ íŒ€ì€ Mercurial í•´ì»¤í†¤ì— ì°¸ì—¬í•˜ì—¬ Mercurialì˜ í™•ì¥ì„±ê³¼ ì»¤ë®¤ë‹ˆí‹°ì˜ ê°œë°©ì„±ì— ê°ëª…ë°›ìŒ

**ì „ì²´ ì—”ì§€ë‹ˆì–´ë§ ì¡°ì§ì˜ ì´ì£¼**

- í˜ì´ìŠ¤ë¶ íŒ€ì€ ë‚˜ë¨¸ì§€ íšŒì‚¬ë¥¼ ì„¤ë“í•˜ê¸° ìœ„í•´ Gitê³¼ Mercurial ê°„ì˜ ëª…ë ¹ì–´ ë° ì›Œí¬í”Œë¡œìš°ë¥¼ ë§¤í•‘í•˜ê³ , ê°œë°œìë“¤ì˜ ìš°ë ¤ë¥¼ ë“£ëŠ” ì‹œê°„ì„ ê°€ì§
- ì´ì£¼ ê³¼ì •ì€ ì‹ ì¤‘í•˜ê²Œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ê²°êµ­ í˜ì´ìŠ¤ë¶ì€ Mercurialë¡œ ì „í™˜í•¨
- í˜ì´ìŠ¤ë¶ì€ Mercurialì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , "stacked diffs"ë¥¼ í†µí•´ ì½”ë“œ ë¦¬ë·° ë³‘ë ¬í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë“±ì˜ ê¸°ì—¬ë¥¼ í•¨

**ë§ˆë¬´ë¦¬ ìƒê°**

- ì´ ì´ì•¼ê¸°ëŠ” "ë§ì€ ì£¼ìš” ê¸°ìˆ ì  ê²°ì •ì€ ê¸°ìˆ ì´ ì£¼ë„í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì‚¬ëŒì´ ì£¼ë„í•œë‹¤"ëŠ” ì ì„ ìƒê¸°ì‹œí‚´
- í˜ì´ìŠ¤ë¶ì€ Mercurialì´ Gitë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ì„œê°€ ì•„ë‹ˆë¼, Mercurial ìœ ì§€ë³´ìˆ˜ìë“¤ê³¼ì˜ í˜‘ì—…ì´ ë” ê°œë°©ì ì´ì—ˆê¸° ë•Œë¬¸ì— ì„ íƒí•¨
- ì „ì²´ ì—”ì§€ë‹ˆì–´ë§ ì¡°ì§ì„ ì„¤ë“í•˜ëŠ” ê³¼ì •ì—ì„œ í•œ ê¸°ìˆ ì´ ë‹¤ë¥¸ ê¸°ìˆ ë³´ë‹¤ ë” ìš°ìˆ˜í•´ì„œê°€ ì•„ë‹ˆë¼ "ì‚¬ë ¤ ê¹Šì€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"ì´ ì¤‘ìš”í–ˆìŒ
- "ì†Œí†µê³¼ ì¹œì ˆí•¨"ì´ ê°œë°œ ë„êµ¬ ì„¸ê³„ì—ì„œ ì¤‘ìš”í•œ ê°€ì¹˜ì„ì„ ê°•ì¡°

[Goë¡œ 10ì–µ í–‰ ì²˜ë¦¬í•˜ê¸° ë„ì „ 9ê°œì˜ ë°©ë²•ìœ¼ë¡œ 1ë¶„45ì´ˆì—ì„œ 4ì´ˆë¡œ ë‹¨ì¶•  GeekNews](https://news.hada.io/topic?id=13627)

- 1BRC : 10ì–µí–‰ì´ ìˆëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì˜¨ë„ ì¸¡ì •ê°’ì„ ì½ì–´ ê´€ì¸¡ì†Œë³„ ìµœì†Œ/í‰ê· /ìµœëŒ€ ì˜¨ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ì±Œë¦°ì§€
- 2024ë…„ 1ì›” 1ì¼ë¶€í„° 1ì›” 31ì¼ê¹Œì§€ ì§„í–‰í–ˆìœ¼ë©°, ìµœì‹  Javaë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ìŒ
- ì´ì— ëŒ€í•´ ì‚¬ëŒë“¤ì´ ê´€ì‹¬ì„ ê°€ì§€ê³  ë‹¤ì–‘í•œ ì–¸ì–´(Rust,Go,C++,SQL)ë¡œ ë„ì „í•˜ê¸° ì‹œì‘
- Goë¡œ ì‘ì„±í•œ 9ê°€ì§€ ì†”ë£¨ì…˜ì— ëŒ€í•´ì„œ ìƒì„¸ ì†Œê°œ (ëŠë¦°ê²ƒë¶€í„° ë¹ ë¥¸ ê²ƒìˆœìœ¼ë¡œ)
- [gunnarmorling1brc 1ï¸âƒ£ğŸğŸï¸ The One Billion Row Challenge -- A fun exploration of how quickly 1B rows from a text file can be aggregated with Java](https://github.com/gunnarmorling/1brc)

**ê¸°ë³¸ ì¸¡ì •ê°’**

- cat ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ 10ì–µí–‰ í…ìŠ¤íŠ¸ë°ì´í„°(13GB) ë°ì´í„°ë¥¼ ì½ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì€ 1.052ì´ˆì„.
- ì‹¤ì œë¡œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” wc ëª…ë ¹ì–´ëŠ” ê±°ì˜ 1ë¶„ì´ ê±¸ë¦¼(55.710ì´ˆ).
- AWK ì†”ë£¨ì…˜ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì€ 7ë¶„ 35ì´ˆì„.

**ì†”ë£¨ì…˜ 1: ê°„ë‹¨í•˜ê³  ê´€ìš©ì ì¸ Go**

- Go í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ì²« ë²ˆì§¸ ì†”ë£¨ì…˜ì€ 1ë¶„ 45ì´ˆê°€ ê±¸ë¦¼.
- bufio.Scannerë¡œ ì¤„ì„ ì½ê³ , strings.Cutìœ¼ë¡œ ';'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•¨.
- strconv.ParseFloatë¡œ ì˜¨ë„ë¥¼ íŒŒì‹±í•˜ê³ , Go ë§µì„ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ëˆ„ì í•¨.

**ì†”ë£¨ì…˜ 2: í¬ì¸í„° ê°’ì´ ìˆëŠ” ë§µ**

- ë§µì—ì„œ ë‘ ë²ˆì˜ í•´ì‹±ì„ í”¼í•˜ê¸° ìœ„í•´ map[string]*statsë¥¼ ì‚¬ìš©í•¨.
- í¬ì¸í„° ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ì„ 1ë¶„ 45ì´ˆì—ì„œ 1ë¶„ 31ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 3: strconv.ParseFloat í”¼í•˜ê¸°**

- strconv.ParseFloat ëŒ€ì‹  ì‚¬ìš©ì ì •ì˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¨ë„ë¥¼ íŒŒì‹±í•¨.
- ì‹œê°„ì„ 1ë¶„ 31ì´ˆì—ì„œ 55.8ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 4: ê³ ì • ì†Œìˆ˜ì  ì •ìˆ˜ ì‚¬ìš©**

- ì˜¨ë„ë¥¼ ì •ìˆ˜ë¡œ í‘œí˜„í•˜ì—¬ ë¶€ë™ ì†Œìˆ˜ì  ì—°ì‚°ì„ í”¼í•¨.
- ì‹œê°„ì„ 55.8ì´ˆì—ì„œ 51.0ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 5: bytes.Cut í”¼í•˜ê¸°**

- ';'ë¥¼ ì°¾ê¸° ìœ„í•´ ì „ì²´ ìŠ¤í…Œì´ì…˜ ì´ë¦„ì„ ìŠ¤ìº”í•˜ëŠ” ëŒ€ì‹  ëì—ì„œë¶€í„° íŒŒì‹±í•¨.
- ì‹œê°„ì„ 51.0ì´ˆì—ì„œ 46.0ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 6: bufio.Scanner í”¼í•˜ê¸°**

- bufio.Scannerë¥¼ ì œê±°í•˜ê³  íŒŒì¼ì„ í° ì²­í¬ë¡œ ì½ìŒ.
- ì‹œê°„ì„ 46.0ì´ˆì—ì„œ 41.3ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 7: ì‚¬ìš©ì ì •ì˜ í•´ì‹œ í…Œì´ë¸”**

- Goì˜ ë§µ ëŒ€ì‹  ì‚¬ìš©ì ì •ì˜ í•´ì‹œ í…Œì´ë¸”ì„ êµ¬í˜„í•¨.
- ì‹œê°„ì„ 41.3ì´ˆì—ì„œ 25.8ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 8: ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬**

- ê°„ë‹¨í•˜ê³  ê´€ìš©ì ì¸ ì½”ë“œë¥¼ ë³‘ë ¬í™”í•˜ì—¬ ì‹œê°„ì„ 1ë¶„ 45ì´ˆì—ì„œ 24.3ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ì†”ë£¨ì…˜ 9: ëª¨ë“  ìµœì í™” ë° ë³‘ë ¬ ì²˜ë¦¬**

- ëª¨ë“  ìµœì í™”ë¥¼ ë³‘ë ¬ ì²˜ë¦¬ì™€ ê²°í•©í•˜ì—¬ ì‹œê°„ì„ 24.3ì´ˆì—ì„œ 3.99ì´ˆë¡œ ë‹¨ì¶•í•¨.

**ê²°ê³¼ í…Œì´ë¸”**

- ëª¨ë“  Go ì†”ë£¨ì…˜ê³¼ ê°€ì¥ ë¹ ë¥¸ Go ë° Java ì†”ë£¨ì…˜ì„ ë¹„êµí•œ í‘œ ì œê³µ.
- Go ë²„ì „ ì¤‘ ê°€ì¥ ë¹ ë¥¸ ê²ƒì€ 2.90ì´ˆ, Java ë²„ì „ì€ 0.953ì´ˆë¡œ ì²˜ë¦¬í•¨.
- 1ì´ˆë„ ì•ˆê±¸ë¦¬ëŠ” Javaë²„ì „ì€ Thomas Wuerthinger(GraalVM ì œì‘ì)ê°€ í•œ ê²ƒìœ¼ë¡œ, ì´ ë¶„ì•¼ ì „ë¬¸ê°€ì´ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œë“¯

**ìµœì¢… ì½”ë©˜íŠ¸**

- ì¼ìƒì ì¸ í”„ë¡œê·¸ë˜ë° ì‘ì—…ì—ì„œëŠ” ê°„ë‹¨í•˜ê³  ê´€ìš©ì ì¸ ì½”ë“œê°€ ì¢‹ì€ ì¶œë°œì ì„.
- ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ê²½ìš°, ì½”ë“œë¥¼ 4ë°° ë˜ëŠ” 26ë°° ë¹ ë¥´ê²Œ ë§Œë“¤ë©´ ì‚¬ìš©ì ë§Œì¡±ë„ë¥¼ ë†’ì´ê³  ì»´í“¨íŒ… ë¹„ìš©ì„ ì ˆì•½í•  ìˆ˜ ìˆìŒ.
- ëŸ°íƒ€ì„ì´ë‚˜ ì¸í„°í”„ë¦¬í„°ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²½ìš°, ì„±ëŠ¥ í–¥ìƒì´ ì¤‘ìš”í•¨.

## 24.03.14

[PEP 20 â€“ The Zen of Python  peps.python.org](https://peps.python.org/pep-0020/)

**íŒŒì´ì¬ì˜ ì„ **

1. ì•„ë¦„ë‹¤ìš´ ê²ƒì´ ì¶”í•œ ê²ƒë³´ë‹¤ ë‚«ë‹¤. (Beautiful is better than ugly)
1. ëª…ì‹œì ì¸ ê²ƒì´ ì•”ì‹œì ì¸ ê²ƒë³´ë‹¤ ë‚«ë‹¤. (Explicit is better than implicit)
1. ë‹¨ìˆœí•œ ê²ƒì´ ë³µì¡í•œ ê²ƒë³´ë‹¤ ë‚«ë‹¤. (Simple is better than complex)
1. ë³µì¡í•œ ê²ƒì´ ë‚œí•´í•œ ê²ƒë³´ë‹¤ ë‚«ë‹¤. (Complex is better than complicated)
1. ê°€ë…ì„±ì€ ì¤‘ìš”í•˜ë‹¤. (Readability counts)
1. íŠ¹ë³„í•œ ê²½ìš°ë“¤ì€ ê·œì¹™ì„ ê¹° ì •ë„ë¡œ íŠ¹ë³„í•˜ì§€ ì•Šë‹¤. (Special cases aren't special enough to break the rules)
1. ì‹¤ìš©ì„±ì´ ìˆœìˆ˜í•¨ì„ ì´ê¸´ë‹¤. (Practicality beats purity)
1. ì˜¤ë¥˜ëŠ” ì ˆëŒ€ë¡œ ì¡°ìš©íˆ ì§€ë‚˜ê°€ì„œëŠ” ì•ˆ ëœë‹¤. (Errors should never pass silently)
1. ëª…ì‹œì ìœ¼ë¡œ ì˜¤ë¥˜ë¥¼ ìˆ¨ê¸°ì§€ ì•ŠëŠ” í•œ. (Unless explicitly silenced)
1. ëª¨í˜¸í•¨ì„ í”¼í•  ìˆ˜ ìˆë‹¤ë©´, ê·¸ ë°©ë²•ì„ ê±°ë¶€í•˜ì§€ ë§ˆë¼. (In the face of ambiguity, refuse the temptation to guess)
1. ë‹¹ì—°íˆ, í•´ê²°ì±…ì´ í•œ ê°€ì§€ë°–ì— ì—†ì–´ì•¼ í•œë‹¤. (There should be one-- and preferably only one --obvious way to do it)
1. ë¹„ë¡ ê·¸ ë°©ë²•ì´ ì²˜ìŒì—ëŠ” ëª…í™•í•˜ì§€ ì•Šë”ë¼ë„. (Although that way may not be obvious at first unless you're Dutch)
1. ì§€ê¸ˆ í•˜ëŠ” ê²ƒì´ ì „í˜€ í•˜ì§€ ì•ŠëŠ” ê²ƒë³´ë‹¤ ë‚«ë‹¤. (Now is better than never)
1. í•˜ì§€ë§Œ ì§€ê¸ˆ ë‹¹ì¥ì€ í•  í•„ìš”ê°€ ì—†ëŠ” ê²ƒë³´ë‹¤ ë‚˜ì¤‘ì´ ë‚«ë‹¤. (Although never is often better than right now)
1. êµ¬í˜„ì„ ì„¤ëª…í•˜ê¸° ì–´ë µë‹¤ë©´, ê·¸ê²ƒì€ ë‚˜ìœ ì•„ì´ë””ì–´ë‹¤. (If the implementation is hard to explain, it's a bad idea)
1. êµ¬í˜„ì„ ì„¤ëª…í•˜ê¸° ì‰½ë‹¤ë©´, ê·¸ê²ƒì€ ì¢‹ì€ ì•„ì´ë””ì–´ì¼ ìˆ˜ ìˆë‹¤. (If the implementation is easy to explain, it may be a good idea)
1. ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì •ë§ ì¢‹ì€ ì•„ì´ë””ì–´ë‹¤! (Namespaces are one honking great idea -- let's do more of those!)

ì´ ì›ì¹™ë“¤ì€ íŒŒì´ì¬ì˜ ì„¤ê³„ ì² í•™ê³¼ ì½”ë“œ ìŠ¤íƒ€ì¼ì„ ë°˜ì˜í•˜ë©°, íŒŒì´ì¬ ì»¤ë®¤ë‹ˆí‹° ë‚´ì—ì„œ ë„ë¦¬ ë°›ì•„ë“¤ì—¬ì§€ê³  ìˆìŠµë‹ˆë‹¤. ëª©ì ì€ ì½”ë“œì˜ ê°€ë…ì„±, ê°„ê²°ì„±, ëª…í™•ì„±ì„ ë†’ì—¬, ì‘ì„±ìê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì‚¬ëŒë„ ì‰½ê²Œ ì´í•´í•˜ê³  ìœ ì§€ë³´ìˆ˜í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë° ìˆìŠµë‹ˆë‹¤.

[Scaling Mercurial at Facebook - Engineering at Meta](https://engineering.fb.com/2014/01/07/core-infra/scaling-mercurial-at-facebook/)

With thousands of commits a week across hundreds of thousands of files, Facebookâ€™s main source repository is enormousâ€“many times larger than even the Linux kernel, which checked in at 17 million lines of code and 44,000 files in 2013. Given our size and complexityâ€”and Facebookâ€™s practice of shipping code twice a dayâ€“improving our source control is one way we help our engineers move fast.

**Choosing a source control system**

We realized that weâ€™d have to solve this ourselves. But instead of building a new system from scratch, we decided to take an existing one and make it scale. Our engineers were comfortable with Git and we preferred to stay with a familiar tool, so we took a long, hard look at improving it to work at scale. After much deliberation, we concluded that Gitâ€™s internals would be difficult to work with for an ambitious scaling project.

Instead, we chose to improve Mercurial. Mercurial is a distributed source control system similar to Git, with many equivalent features. Importantly, itâ€™s written mostly in clean, modular Python (with some native code for hot paths), making it deeply extensible. Just as importantly, the Mercurial developer community is actively helping us address our scaling problems by reviewing our patches and keeping our scale in mind when designing new features.

When we first started working on Mercurial, we found that it was slower than Git in several notable areas. To narrow this performance gap, weâ€™ve contributed over 500 patches to Mercurial over the last year and a half. These range from new graph algorithms to rewrites of tight loops in native code. These helped, but we also wanted to make more fundamental changes to address the problem of scale.

**Speeding up file status operations**

For a repository as large as ours, **a major bottleneck is simply finding out what files have changed. Git examines every file and naturally becomes slower and slower as the number of files increases,** while Perforce â€œcheatsâ€ by forcing users to tell it which files they are going to edit. The Git approach doesnâ€™t scale, and the Perforce approach isnâ€™t friendly.

**We solved this by monitoring the file system for changes.** This has been tried before, even for Mercurial, but making it work reliably is surprisingly challenging. **We decided to query our build systemâ€™s file monitor, Watchman**, to see which files have changed. Mercurialâ€™s design made integrating with Watchman straightforward, but we expected Watchman to have bugs, so we developed a strategy to address them safely.

For our repository, **enabling Watchman integration has made Mercurialâ€™s status command more than 5x faster than Gitâ€™s status command.** Other commands that look for changed filesâ€“like diff, update, and commitâ€”also became faster.

**Working with large histories**

The rate of commits and the sheer size of our history also pose challenges. We have thousands of commits being made every day, and as the repository gets larger, it becomes increasingly painful to clone and pull all of it. Centralized source control systems like Subversion avoid this by only checking out a single commit, leaving all of the history on the server. This saves space on the client but leaves you unable to work if the server goes down. **More recent distributed source control systems, like Git and Mercurial, copy all of the history to the client which takes more time and space, but allows you to browse and commit entirely locally.** We wanted a happy medium between the speed and space of a centralized system and the robustness and flexibility of a distributed one.

**Improving clone and pull**

Normally when you run a pull, Mercurial figures out what has changed on the server since the last pull and downloads any new commit metadata and file contents. With tens of thousands of files changing every day, downloading all of this history to the client every day is slow. **To solve this problem we created the remotefilelog extension for Mercurial. This extension changes the clone and pull commands to download only the commit metadata**, while **omitting all file changes that account for the bulk of the download**. When a user performs an operation that needs the contents of files (such as checkout), we download the file contents on demand using Facebookâ€™s existing memcache infrastructure. This allows clone and pull to be fast no matter how much history has changed, while only adding a slight overhead to checkout.

But what if the central Mercurial server goes down? A big benefit of distributed source control is the ability to work without interacting with the server. **The remotefilelog extension intelligently caches the file revisions needed for your local commits so you can checkout, rebase, and commit to any of your existing bookmarks without needing to access the server. Since we still download all of the commit metadata, operations that donâ€™t require file contents (such as log) are completely local as well.** Lastly, we use Facebookâ€™s memcache infrastructure as a caching layer in front of the central Mercurial server, so that even if the central repository goes down, memcache will continue to serve many of the file content requests.

**Clone and pull performance gains**

Enabling the remotefilelog extension for employees at Facebook has made **Mercurial clones and pulls 10x faster**, bringing them down from minutes to seconds. In addition, because of the way remotefilelog stores its local data on disk, **large rebases are 2x faster.** When compared with our previous Git infrastructure, the numbers remain impressive. Achieving these types of performance gains through extensions is one of the big reasons we chose Mercurial.

- [FsMonitorExtension - Mercurial](https://wiki.mercurial-scm.org/FsMonitorExtension)
  - This extension was previously known as hgwatchman before being merged into Mercurial.

## 24.03.19

[Frozen collections in .NET 8](chrome-extension://hajanaajapkhaabfcofdjgjnlgkdkknm/)

- .NET8ì˜ Frozen Collection ì˜ ì„±ëŠ¥ì€ ì¼ë°˜ ì»¬ë ‰ì…˜ ë„êµ¬ì™€ Immutable ì»¬ë ‰ì…˜ ë„êµ¬ë³´ë‹¤ ë¹ ë¥´ë‹¤.
- Immutable ì»¬ë ‰ì…˜ ë„êµ¬ë“¤ì´ ìƒê°ë³´ë‹¤ ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì•˜ë‹¤. ì´ìœ ëŠ” í¬ìŠ¤íŠ¸ì— ë‚˜ì™€ìˆì§€ ì•Šì§€ë§Œ, ì´ê²ƒë„ .net 7ì— ë¹„í•´ ê°œì„ ëœ ê²ƒì´ë¼ê³ 

[Don't Block on Async Code](https://blog.stephencleary.com/2012/07/dont-block-on-async-code.html)

> ASP.NET Example

```cs
// My "library" method.
public static async Task<JObject> GetJsonAsync(Uri uri)
{
  // (real-world code shouldn't use HttpClient in a using block; this is just example code)
  using (var client = new HttpClient())
  {
    var jsonString = await client.GetStringAsync(uri);
    return JObject.Parse(jsonString);
  }
}

// My "top-level" method.
public class MyController : ApiController
{
  public string Get()
  {
    var jsonTask = GetJsonAsync(...);
    return jsonTask.Result.ToString();
  }
}
```

So this is what happens, starting with the top-level method (Button1_Click for UI / MyController.Get for ASP.NET):

1. The top-level method calls GetJsonAsync (within the UI/ASP.NET context).
1. GetJsonAsync starts the REST request by calling HttpClient.GetStringAsync (still within the context).
1. GetStringAsync returns an uncompleted Task, indicating the REST request is not complete.
1. GetJsonAsync awaits the Task returned by GetStringAsync. The context is captured and will be used to continue running the GetJsonAsync method later. GetJsonAsync returns an uncompleted Task, indicating that the GetJsonAsync method is not complete.
1. The top-level method synchronously blocks on the Task returned by GetJsonAsync. This blocks the context thread.
1. â€¦ Eventually, the REST request will complete. This completes the Task that was returned by GetStringAsync.
1. The continuation for GetJsonAsync is now ready to run, and it waits for the context to be available so it can execute in the context.
1. Deadlock. The top-level method is blocking the context thread, waiting for GetJsonAsync to complete, and GetJsonAsync is waiting for the context to be free so it can complete.

**Preventing Deadlock**

There are two best practices (both covered in my intro post) that avoid this situation:

1. In your â€œlibraryâ€ async methods, use ConfigureAwait(false) wherever possible.
2. Donâ€™t block on Tasks; use async all the way down.

```cs
public static async Task<JObject> GetJsonAsync(Uri uri)
{
  // (real-world code shouldn't use HttpClient in a using block; this is just example code)
  using (var client = new HttpClient())
  {
    var jsonString = await client.GetStringAsync(uri).ConfigureAwait(false);
    return JObject.Parse(jsonString);
  }
}
```

This changes the continuation behavior of GetJsonAsync so that it does not resume on the context. Instead, GetJsonAsync will resume on a thread pool thread. This enables GetJsonAsync to complete the Task it returned without having to re-enter the context. The top-level methods, meanwhile, do require the context, so they cannot use ConfigureAwait(false).

```cs
public async void Button1_Click(...)
{
  var json = await GetJsonAsync(...);
  textBox1.Text = json;
}

public class MyController : ApiController
{
  public async Task<string> Get()
  {
    var json = await GetJsonAsync(...);
    return json.ToString();
  }
}
```

- [.NET Framework 394. asyncawait ì‚¬ìš© ì‹œ hang ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš°](https://www.sysnet.pe.kr/2/0/1541)
- [ë‹·ë„· 2224. C - WPFì˜ Dispatcher Queueë¡œ ì•Œì•„ë³´ëŠ” await í˜¸ì¶œì˜ hang í˜„ìƒ](https://www.sysnet.pe.kr/2/0/13572#SetOnInvokeMres)

```cs
private void Window_Loaded(object sender, RoutedEventArgs e)
{
    Task<string> task = GetMyText();
    this.textBox1.Text = task.Result; // ë¬´í•œ ëŒ€ê¸°!!!
}

async Task<string> GetMyText()
{
    await Task.Delay(1);
    return "Hello World";
}
```

## 24.03.20

[Unity Mathematics  Mathematics  1.3.2](https://docs.unity3d.com/Packages/com.unity.mathematics@1.3/manual/index.html)

- [Unity-TechnologiesUnity.Mathematics The C math library used in Unity providing vector types and math functions with a shader like syntax](https://github.com/Unity-Technologies/Unity.Mathematics)
- Unity ì˜ ë²„ìŠ¤íŠ¸ ì»´íŒŒì¼ëŸ¬ë¥¼ í™œìš©. ìˆ˜í•™ ì—°ì‚° íƒ€ì…ë“¤ì˜ í•˜ë“œì›¨ì–´ ê°€ì† ì§€ì›
  - ê·¸ì¹˜ë§Œ Unity.Mathmatics ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê³µë˜ëŠ” íƒ€ì…ì„ í™œìš©í•´ì•¼ í•˜ê³  + AOT ì»´íŒŒì¼ í™˜ê²½ì´ì–´ì•¼ í•¨
  - ë‹·ë„· ìƒíƒœê³„ì˜ System.Numerics íƒ€ì…ë“¤ì€ ì§€ì›í•´ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì–˜ê¸°
- System.Numerics ì˜ ê²½ìš°ì—ëŠ” ë˜ JIT ì»´íŒŒì¼ í™˜ê²½ì—ì„œ í•˜ë“œì›¨ì–´ ê°€ì†ì„ ì œê³µí•¨
- í˜„ì¬ í”„ë¡œì íŠ¸ê°€ ì„œë²„ì™€ í´ë¼ê°€ ë™ì¼í•œ c# ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ê³ , ì´ê±¸ë¡œ ì„œë²„ í´ë¼ ê°„ì˜ ê³µìš© ì½”ë“œ ê³µê°„ì„ êµ¬ì„±í•  ìˆ˜ ìˆì—ˆì§€ë§Œ
  - ìˆ˜í•™ ì½”ë“œì— ëŒ€í•´ì„œëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ ê³¨ì¹˜ ì•„í”„ê²Œ ë¨.
  - AOT ì»´íŒŒì¼ í™˜ê²½ì˜ ìœ ë‹ˆí‹°ì—ì„œëŠ” Unity.Mathmaticsë¡œ í•˜ë“œì›¨ì–´ ê°€ì† ì§€ì›ì´ í•„ìˆ˜ì§€ë§Œ
  - JIT ì»´íŒŒì¼ í™˜ê²½ì˜ ì„œë²„ì—ì„œëŠ” ì €ê±¸ ì‚¬ìš©í•  ì´ìœ ê°€ ì—†ëŠ” ìƒíƒœ

## 24.03.21

[C ì½”ë”© í‘œì¤€  í¬í”„ì˜ ë¬¸ì„œì°½ê³ ](https://docs.popekim.com/ko/coding-standards/csharp)

ê¸°ë³¸ ì›ì¹™

1. ê°€ë…ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ ì‚¼ëŠ”ë‹¤. (ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì½”ë“œëŠ” ê·¸ ìì²´ê°€ ë¬¸ì„œì˜ ì—­í• ì„ í•´ì•¼ í•¨)
2. ì •ë§ í•©ë‹¹í•œ ì´ìœ ê°€ ìˆì§€ ì•ŠëŠ” í•œ, í†µí•©ê°œë°œí™˜ê²½(IDE)ì˜ ìë™ ì„œì‹ì„ ë”°ë¥¸ë‹¤. (ë¹„ì£¼ì–¼ ìŠ¤íŠœë””ì˜¤ì˜ "Ctrl + K + D" ê¸°ëŠ¥)
3. ë³¸ ì½”ë”©í‘œì¤€ì„ ë”°ë¼ ì˜ ì§œì—¬ì§„ ê¸°ì¡´ì˜ ì½”ë“œì—ì„œ ë°°ìš´ë‹¤.

[Coding Standard  Epic Developer Community](chrome-extension://hajanaajapkhaabfcofdjgjnlgkdkknm/documentation/assets/)

At Epic Games, we have a few simple coding standards and conventions. This document reflects the state of Epic Games' current coding standards. Following the coding standards is mandatory.

Code conventions are important to programmers for several reasons:

- 80% of the lifetime cost of a piece of software goes to maintenance.
- Hardly any software is maintained for its whole life by the original author.
- Code conventions improve the readability of software, allowing engineers to understand new code quickly and thoroughly.
- If we decide to expose source code to mod community developers, we want it to be easily understood.
- Many of these conventions are required for cross-compiler compatibility.

[(81) Jonathan Blow on the Problem with Object Oriented - YouTube](https://www.youtube.com/watch?v=04ksL1hf_p8&list=WL&index=5)

- ìë™ ìƒì„± ì˜ì–´ ìë§‰ì„ ë”°ì˜´. ì•„ì €ì”¨ê°€ ë§ì„ íš¡ì„¤ìˆ˜ì„¤í•´ì„œ ìë§‰ ë¬¸ì¥ì´ ì–´ìƒ‰í•¨.

> here's actually what one of the big problems with software right is that starting in the 80s prior to the ' 80s everybody was just trying to solve real problems
> 
> and trying to get computers to solve real problems right and then starting around the 80s computers got fast enough that you had a lot more leeway around how you solve the problems 
> 
> and people started making all these theories about how to uh how to solve problems in the best way and so objectoriented programming is like one of those theories 
> 
> right oh on the list of things in C++ that are wrong object oriented is wrong right because of

- ê³¼ê±° 80ë…„ëŒ€ì—ëŠ” ë¬¼ë¦¬ ì„¸ê³„ì˜ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œ ì»´í“¨í„°ë¼ëŠ” ìì›ì„ ì‚¬ìš©. ê·¸ ì‹œì ˆì— object-oriented ê°œë…ì´ ì—†ì—ˆë‹¤.

> in C++ object-oriented World representation style then you start to believe from the way that you're taught that programming work is about building the hierarchy of objects in the right way or something but that's not actually solving a problem it's just conforming to a belief structure that
>
> you've been given after you've decided on that hierarchy of objects you still don't have a program that solves any problems now you have to go write the program within that framework that you decided on right and so the problem is as we get further and further away from real actual problems more and more of what is taught to people as programming is actually not doing anything
>
> it's just shuffling papers around on your desk right and so like the question of how do I do an interface that blah blah blah is just shuffling the papers around on your desk because it's not actually solving a problem like what I would say is write the editor and then write the plugins or whatever write the functionality make the functionality work and then if you're dispatching between them with an if statement and it feels kind of hacky and you need more abstraction for a very concrete reason than put in a function pointer but I wouldn't even do that I would just have an if statement that turns them on or off for a long time because it's it's actually a relatively that part of writing an editor is so small and so minor in terms of what it solves that I wouldn't even bother for a long time

- OOP ëŠ” ë¬¸ì œë¥¼ ì˜ í’€ê¸° ìœ„í•œ ì´ë¡  ì¤‘ì— í•˜ë‚˜ì§€ë§Œ
- ì‹¤ì œ ë¬¸ì œ í•´ê²°ì—ì„œ ë©€ì–´ì ¸ë²„ë¦° ë“¯
  - oop ì´ë¡  ì´í›„, ìš°ë¦¬ëŠ” ê°ì²´ì˜ ì¶”ìƒí™”(abstraction)ë¥¼ ìƒê°í•˜ê³  ì˜ì¡´ì„±(dependency)ë¥¼ ìƒê°í•˜ê³  ì¸í„°í˜ì´ìŠ¤(interface)ë¥¼ ìƒê°í•˜ê³  ìˆ˜ë§ì€ í”ŒëŸ¬ê·¸ì¸ê³¼ í”„ë ˆì„ì›Œí¬ì™€ ë²„ì „ì„ ë¨¸ë¦¿ì†ì— ë„£ê³  ì‘ì—…í•´ì•¼ í•¨.
  - í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ê±°ëŒ€í•œ ê°ì²´ ì—°ê²° ì‹œìŠ¤í…œì„ ë¨¼ì € ìƒê°í•´ì•¼í•¨.
  - ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ê°€ ì‹¤ì œë¡œ í’€ê³ ì í–ˆë˜ ë¬¸ì œëŠ” ì–´ë””ë¡œ ê°„ ê²ƒì¸ì§€...?? ì¼ì„ ìœ„í•œ ì¼ì„ ë§Œë“¤ê³  ìˆëŠ” ê²ƒì´ ì•„ë‹Œì§€??

## 24.03.25

[Performance Improvements in .NET 7 - .NET Blog](https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#jit)

**Tiered Compiliation**

- ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œë  ìˆ˜ ìˆëŠ” ì½”ë“œì— ëŒ€í•´ì„œ JIT ì»´íŒŒì¼ëŸ¬ëŠ” ì²« í˜¸ì¶œì‹œì—ëŠ” ë‚®ì€ ìµœì í™” ìˆ˜ì¤€ì˜ ê¸°ê³„ì–´ë¡œ, ì´í›„ ìì£¼ í˜¸ì¶œëœë‹¤ë©´ ë†’ì€ ìµœì í™” ìˆ˜ì¤€ì˜ ê¸°ê³„ì–´ë¡œ ì»´íŒŒì¼í•´ì£¼ëŠ” ë°©ì‹
- ë†’ì€ ìµœì í™” ìˆ˜ì¤€ìœ¼ë¡œ ì»´íŒŒì¼í•˜ë ¤ë©´ ì‹œê°„ì´ ì˜¤ë˜ê±¸ë¦¬ê¸° ë•Œë¬¸ì—, í•œë²ˆ í˜¸ì¶œë˜ê³  ë§ˆëŠ” ì½”ë“œì— ëŒ€í•´ì„œëŠ” ê³ ìˆ˜ì¤€ ìµœì í™”ê°€ í•„ìš” ì—†ì„ ê²ƒì´ë¼ëŠ” ê°€ì •ì— ê¸°ë°˜

-  On-stack replacement (OSR) is one of the coolest features to hit the JIT in .NET 7. But to really understand OSR, we first need to understand tiered compilation, so a quick recapâ€¦
- One of the issues a managed environment with a JIT compiler has to deal with is tradeoffs between startup and throughput. Historically, the job of an optimizing compiler is to, well, optimize, in order to enable the best possible throughput of the application or service once running. But **such optimization takes analysis, takes time, and performing all of that work then leads to increased startup time**, as all of the code on the startup path (e.g. all of the code that needs to be run before a web server can serve the first request) needs to be compiled. **So a JIT compiler needs to make tradeoffs: better throughput at the expense of longer startup time, or better startup time at the expense of decreased throughput**. For some kinds of apps and services, the tradeoff is an easy call, e.g. if your service starts up once and then runs for days, several extra seconds of startup time doesnâ€™t matter, or if youâ€™re a console application thatâ€™s going to do a quick computation and exit, startup time is all that matters. But how can the JIT know which scenario itâ€™s in, and do we really want every developer having to know about these kinds of settings and tradeoffs and configure every one of their applications accordingly? One answer to this has been ahead-of-time compilation, which has taken various forms in .NET. For example, all of the core libraries are â€œcrossgenâ€â€˜d, meaning theyâ€™ve been run through a tool that produces the previously mentioned R2R format, yielding binaries that contain assembly code that needs only minor tweaks to actually execute; not every method can have code generated for it, but enough that it significantly reduces startup time. Of course, such approaches have their own downsides, e.g. one of the promises of a JIT compiler is it can take advantage of knowledge of the current machine / process in order to best optimize, so for example the R2R images have to assume a certain baseline instruction set (e.g. what vectorizing instructions are available) whereas the JIT can see whatâ€™s actually available and use the best. â€œTiered compilationâ€ provides another answer, one thatâ€™s usable with or without these other ahead-of-time (AOT) compilation solutions.
- Tiered compilation enables the JIT to have its proverbial cake and eat it, too. The idea is simple: allow the JIT to compile the same code multiple times. The first time, the JIT can use as a few optimizations as make sense (a handful of optimizations can actually make the JITâ€™s own throughput faster, so those still make sense to apply), producing fairly unoptimized assembly code but doing so really quickly. And when it does so, it can add some instrumentation into the assembly to track how often the methods are called. As it turns out, many functions used on a startup path are invoked once or maybe only a handful of times, and it would take more time to optimize them than it does to just execute them unoptimized. Then, when the methodâ€™s instrumentation triggers some threshold, for example a method having been executed 30 times, a work item gets queued to recompile that method, but this time with all the optimizations the JIT can throw at it. This is lovingly referred to as â€œtiering up.â€ Once that recompilation has completed, call sites to the method are patched with the address of the newly highly optimized assembly code, and future invocations will then take the fast path. So, we get faster startup and faster sustained throughput. At least, thatâ€™s the hope.

[Performance Improvements in .NET 8 - .NET Blog](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/)

- One wrinkle to this scheme, however, is the presence of longer-running methods. Methods might be important because theyâ€™re invoked many times, but they might also be important because theyâ€™re invoked only a few times but end up running forever, in particular due to looping.
  - ì´ ê°œë…ì—ì„œ ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ë‹¨ì ì€, êµ‰ì¥íˆ ë“œë¬¼ê²Œ ì‹¤í–‰ë˜ì§€ë§Œ í•œë²ˆ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ êµ‰ì¥íˆ ë¬´ê²ê±°ë‚˜ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” ë©”ì„œë“œë“¤ì´ ê³ ìˆ˜ì¤€ ìµœì í™” í˜œíƒì„ ë°›ì§€ ëª»í•˜ê²Œ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.
  - ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ while ë£¨í”„ë¥¼ ëì—†ì´ ëŒë¦¬ëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ê¸°ì— í•´ë‹´ë¨
- As such, tiering was disabled by default for methods containing backward branches, such that those methods would go straight to tier 1. To address that, .NET 7 introduced On-Stack Replacement (OSR). With OSR, the code generated for loops also included a counting mechanism, and after a loop iterated to a certain threshold, the JIT would compile a new optimized version of the method and jump from the minimally-optimized code to continue execution in the optimized variant. Pretty slick, and with that, in .NET 7 tiering was also enabled for methods with loops.
  - ì´ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ì„œ .NET 7 ì€ OSR(On-Stack Replacement) ê°œë…ì„ ë„ì….
  - ë£¨í”„ê°€ íŠ¹ì • ì„ê³„ê°’ê¹Œì§€ ë°˜ë³µë˜ë©´ JITê°€ ìµœì í™”ëœ ìƒˆ ë²„ì „ì˜ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒ. ì €ìˆ˜ì¤€ ê¸°ê³„ì–´ ì½”ë“œì—ì„œ -> ê³ ìˆ˜ì¤€ ê¸°ê³„ì–´ ì½”ë“œë¡œ ì í”„í•´ì„œ ì‹¤í–‰ì„ ì§€ì†. (ì´ë•Œ ë©”ì„œë“œ ì‹¤í–‰ì„ ìœ„í•œ ìì›ì€ stack ì— ìˆëŠ” ìƒíƒœì´ë‹ˆ on stack replacement)
- The idea behind on-stack replacement is a method can be replaced not just between invocations but even while itâ€™s executing, while itâ€™s â€œon the stack.â€ In addition to the tier-0 code being instrumented for call counts, loops are also instrumented for iteration counts. When the iterations surpass a certain limit, the JIT compiles a new highly optimized version of that method, transfers all the local/register state from the current invocation to the new invocation, and then jumps to the appropriate location in the new method.

## 24.03.27

[í”„ë¡œê·¸ë˜ë° ëŒ€íšŒ ì°¸ì—¬ì‹œ ë§Œë‚˜ëŠ” ìƒëŒ€  ìœ ë¨¸ ê²Œì‹œíŒ  RULIWEB](https://m.ruliweb.com/best/board/300143/read/65427663)

![](img/2024-03-27-21-55-23.png)

## 24.04.15

[(129) What can I make with C - YouTube](https://www.youtube.com/watch?v=P_6MQZpXgrc)

- c# ìœ¼ë¡œ ë­˜ í•  ìˆ˜ ìˆë‚˜ìš”
  - ë‹¹ë‡¨ë¥¼ ê°€ì§„ ê°œë°œìê°€ ìì‹ ì˜ ì¸ê³µ ì·Œì¥ì— c# ìœ¼ë¡œ ì‘ì„±ëœ ì½”ë“œë¥¼ ì ìš©. RestAPI ë¥¼ í˜¸ì¶œí•œë‹¤ê³  í•¨

![](img/2024-04-15-23-34-38.png)

![](img/2024-04-15-23-35-32.png)

## 24.04.16

**ë‹·ë„·ì˜ ìµœì í™” ê¸°ìˆ ë“¤ ì˜ˆì‹œ**

[dotNET ReadToRun Compilation (R2R)](https://learn.microsoft.com/en-gb/dotnet/core/deploying/ready-to-run)

> R2R binaries improve startup performance by reducing the amount of work the just-in-time (JIT) compiler needs to do as your application loads. The binaries contain similar native code compared to what the JIT would produce. However, R2R binaries are larger because they contain both intermediate language (IL) code, which is still needed for some scenarios, and the native version of the same code. R2R is only available when you publish an app that targets specific runtime environments (RID) such as Linux x64 or Windows x64.

- JIT ê°€ ëŸ°íƒ€ì„ì— í•  ì¼ì„ ë¯¸ë¦¬ í•´ì£¼ê¸° (precompiled/pregenerated code)
  - JIT ê°€ ëŸ°íƒ€ì„ì— ìƒì„±í•˜ëŠ” ê²°ê³¼ë¬¼ì„ ë¯¸ë¦¬ ë§Œë“¤ì–´ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œë“œ ë‹¨ê³„ì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒ
- í•˜ì§€ë§Œ ì½”ë“œ ì‚¬ì´ì¦ˆê°€ ì»¤ì§„ë‹¤. IL ì½”ë“œë„ ê°™ì´ í¬í•¨ë˜ëŠ”ë° ì¼ë¶€ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í•„ìš”í•´ì„œë¼ê³ 
- Linux x64 / Windows x64 ë“± ë°°í¬ ê°€ëŠ¥í•œ ëŸ°íƒ€ì„ì´ ì œí•œëœë‹¤.

> Ahead-of-time compilation has complex performance impact on application performance, which can be difficult to predict. In general, the size of an assembly will grow to between two to three times larger. This increase in the physical size of the file may reduce the performance of loading the assembly from disk, and increase working set of the process. However, in return the number of methods compiled at run time is typically reduced substantially. The result is that most applications that have large amounts of code receive large performance benefits from enabling ReadyToRun. Applications that have small amounts of code will likely not experience a significant improvement from enabling ReadyToRun, as the .NET runtime libraries have already been precompiled with ReadyToRun.

  AOT ì»´íŒŒì¼ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—, ë¹Œë“œ ê²°ê³¼ íŒŒì¼ ì‚¬ì´ì¦ˆê°€ ì»¤ì§„ë‹¤. ì–´ì…ˆë¸”ë¦¬ê°€ ì»¤ì§€ê³  í”„ë¡œì„¸ìŠ¤ì˜ Working Set ì´ ëŠ˜ì–´ë‚œë‹¤.
  ëŸ°íƒ€ì„ ì¤‘ì— ì»´íŒŒì¼ì„ ê±°ì˜ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ íšê¸°ì ìœ¼ë¡œ í–¥ìƒëœë‹¤.

[performance_improvements_in_net_7/#pgo)](https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#pgo)

- PGO: Profile Guided Optimization
- [runtimedocsdesignfeaturesDynamicPgo.md at main Â· dotnetruntime](https://github.com/dotnet/runtime/blob/main/docs/design/features/DynamicPgo.md)

> PGO has been around for a long time, in any number of languages and compilers. The basic idea is you compile your app, asking the compiler to inject instrumentation into the application to track various pieces of interesting information. You then put your app through its paces, running through various common scenarios, causing that instrumentation to â€œprofileâ€ what happens when the app is executed, and the results of that are then saved out. The app is then recompiled, feeding those instrumentation results back into the compiler, and allowing it to optimize the app for exactly how itâ€™s expected to be used. This approach to PGO is referred to as â€œstatic PGO,â€ as the information is all gleaned ahead of actual deployment, and itâ€™s something .NET has been doing in various forms for years.
>
> Dynamic PGO takes advantage of tiered compilation. I noted that the JIT instruments the tier-0 code to track how many times the method is called, or in the case of loops, how many times the loop executes. It can instrument it for other things as well. For example, it can track exactly which concrete types are used as the target of an interface dispatch, and then in tier-1 specialize the code to expect the most common types (this is referred to as â€œguarded devirtualization,â€ or GDV).

[performance_improvements_in_net_7/#bounds-check-elimination](https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#bounds-check-elimination)

> One of the things that makes .NET attractive is its safety. The runtime guards access to arrays, strings, and spans such that you canâ€™t accidentally corrupt memory by walking off either end; if you do, rather than reading/writing arbitrary memory, youâ€™ll get exceptions. Of course, thatâ€™s not magic; itâ€™s done by the JIT inserting bounds checks every time one of these data structures is indexed.

[performance_improvements_in_net_7/#loop-hoisting-and-cloning](https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#loop-hoisting-and-cloning)

ChatGPT: í”„ë¡œê·¸ë˜ë°ì—ì„œ "loop hoisting"ì€ ì¼ë°˜ì ìœ¼ë¡œ ì½”ë“œ ìµœì í™” ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ë°˜ë³µë¬¸ì˜ ì‹¤í–‰ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê¸°ë²•ì€ ë°˜ë³µë¬¸ ë‚´ë¶€ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì§€ë§Œ ë°˜ë³µë§ˆë‹¤ ê²°ê³¼ê°€ ë³€í•˜ì§€ ì•ŠëŠ” ì—°ì‚°ì´ë‚˜ ëª…ë ¹ì„ ë°˜ë³µë¬¸ì˜ ì™¸ë¶€ë¡œ ì´ë™ì‹œí‚¤ëŠ” ê³¼ì •ì„ ë§í•©ë‹ˆë‹¤. ì¦‰, ë°˜ë³µë¬¸ì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ë¶ˆí•„ìš”í•˜ê²Œ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰ë˜ëŠ” ì—°ì‚°ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

[performance_improvements_in_net_7/#folding-propagation-and-substitution](https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#folding-propagation-and-substitution)

- ìƒìˆ˜ í‘œí˜„ ì••ì¶•(folding)
  
ìƒìˆ˜ ì „íŒŒ(Propagation)

> Constant propagation is intricately linked to constant folding and is essentially just the idea that you can substitute a constant value (typically one computed via constant folding) into further expressions, at which point they may also be able to be folded.

- ìƒìˆ˜ ì „íŒŒëŠ” ìƒìˆ˜ ì ‘ê¸°ì™€ ë³µì¡í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ë³¸ì§ˆì ìœ¼ë¡œ ìƒìˆ˜ ê°’(ì¼ë°˜ì ìœ¼ë¡œ ìƒìˆ˜ ì ‘ê¸°ë¥¼ í†µí•´ ê³„ì‚°ëœ ê°’)ì„ ë‹¤ë¥¸ í‘œí˜„ì‹ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìœ¼ë©°, ê·¸ ì‹œì ì—ì„œ ì ‘ì„ ìˆ˜ë„ ìˆë‹¤ëŠ” ì•„ì´ë””ì–´ì— ë¶ˆê³¼í•©ë‹ˆë‹¤.

[performance_improvements_in_net_7/#inlining](https://devblogs.microsoft.com/dotnet/performance_improvements_in_net_7/#inlining)

inlining ì´ ë©”ì„œë“œ í˜¸ì¶œ ì˜¤ë²„í—¤ë“œë¥¼ ì œê±°í•˜ê²Œ í•´ì¤Œ. ë©”ì„œë“œ ì½”ë“œ ì‚¬ì´ì¦ˆê°€ ì§§ë‹¤ë©´ ì¸ë¼ì´ë‹ì˜ í˜œíƒì„ ë°›ê¸° ì¢‹ìŒ.

> The bigger wins are due to the calleeâ€™s code being exposed to the callerâ€™s code, and vice versa. So, for example, if the caller is passing a constant as an argument to the callee, if the method isnâ€™t inlined, the compilation of the callee has no knowledge of that constant, but if the callee is inlined, all of the code in the callee is then aware of its argument being a constant value, and can do all of the optimizations possible with such a constant, like dead code elimination, branch elimination, constant folding and propagation, and so on.

- ë” í° ì´ì ì€ í˜¸ì¶œìì˜ ì½”ë“œê°€ í˜¸ì¶œìì˜ ì½”ë“œì— ë…¸ì¶œë˜ê¸° ë•Œë¬¸ì´ë©°, ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í˜¸ì¶œìê°€ í˜¸ì¶œìì—ê²Œ ìƒìˆ˜ë¥¼ ì¸ìë¡œ ì „ë‹¬í•  ë•Œ ë©”ì„œë“œê°€ ì¸ë¼ì¸ì´ ì•„ë‹Œ ê²½ìš° í˜¸ì¶œìì˜ ì»´íŒŒì¼ì€ í•´ë‹¹ ìƒìˆ˜ë¥¼ ì•Œì§€ ëª»í•˜ì§€ë§Œ í˜¸ì¶œìê°€ ì¸ë¼ì¸ì´ë©´ í˜¸ì¶œìì˜ ëª¨ë“  ì½”ë“œê°€ í•´ë‹¹ ì¸ìˆ˜ê°€ ìƒìˆ˜ ê°’ì„ì„ ì¸ì‹í•˜ê³  ì£½ì€ ì½”ë“œ ì œê±°, ë¶„ê¸° ì œê±°, ìƒìˆ˜ ì ‘ê¸° ë° ì „íŒŒ ë“±ê³¼ ê°™ì´ í•´ë‹¹ ìƒìˆ˜ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  ìµœì í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

ì•„ë˜ëŠ” ìºì‹œì— ëŒ€í•œ ê´€ì 

>  As with any cache, the more times you need to read from memory to populate it, the less effective the cache will be. If you have a function that gets inlined into 100 different call sites, every one of those call sitesâ€™ copies of the calleeâ€™s instructions are unique, and calling each of those 100 functions could end up thrashing the instruction cache; in contrast, if all of those 100 functions â€œsharedâ€ the same instructions by simply calling the single instance of the callee, itâ€™s likely the instruction cache would be much more effective and lead to fewer trips to memory.

- ë‹¤ë¥¸ ìºì‹œì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ìºì‹œë¥¼ ì±„ìš°ê¸° ìœ„í•´ ë©”ëª¨ë¦¬ì—ì„œ ì½ì–´ì•¼ í•˜ëŠ” íšŸìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ìºì‹œì˜ íš¨ìœ¨ì„±ì€ ë–¨ì–´ì§‘ë‹ˆë‹¤. 100ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í˜¸ì¶œ ì‚¬ì´íŠ¸ë¡œ ì¸ë¼ì¸ë˜ëŠ” í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ê° í˜¸ì¶œ ì‚¬ì´íŠ¸ì˜ í˜¸ì¶œì ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë³µì‚¬ë³¸ì€ ëª¨ë‘ ê³ ìœ í•˜ë¯€ë¡œ 100ê°œì˜ í•¨ìˆ˜ë¥¼ ê°ê° í˜¸ì¶œí•˜ë©´ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ìºì‹œê°€ ë‚­ë¹„ë  ìˆ˜ ìˆì§€ë§Œ, ë°˜ëŒ€ë¡œ 100ê°œì˜ ëª¨ë“  í•¨ìˆ˜ê°€ í˜¸ì¶œìì˜ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ë™ì¼í•œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ì„ 'ê³µìœ 'í•œë‹¤ë©´ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ìºì‹œê°€ í›¨ì”¬ ë” íš¨ê³¼ì ì´ê³  ë©”ëª¨ë¦¬ë¡œ ì´ë™í•˜ëŠ” íšŸìˆ˜ê°€ ì¤„ì–´ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[performance-improvements-in-net-8/#branching](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/#branching)

Branching is integral to all meaningful code; while some algorithms are written in a branch-free manner, branch-free algorithms typically are challenging to get right and complicated to read, and typically are isolated to only small regions of code. For everything else, branching is the name of the game. Loops, if/else blocks, ternariesâ€¦ itâ€™s hard to imagine any real code without them. Yet they can also represent one of the more significant costs in an application. Modern hardware gets big speed boosts from pipelining, for example from being able to start reading and decoding the next instruction while the previous ones are still processing. That, of course, relies on the hardware knowing what the next instruction is. If thereâ€™s no branching, thatâ€™s easy, itâ€™s whatever instruction comes next in the sequence. For when there is branching, CPUs have built-in support in the form of branch predictors, used to determine what the next instruction most likely will be, and theyâ€™re often rightâ€¦ but when theyâ€™re wrong, the cost incurred from that incorrect branch prediction can be huge. Compilers thus strive to minimize branching.

- ë¸Œëœì¹˜ëŠ” ëª¨ë“  ì˜ë¯¸ ìˆëŠ” ì½”ë“œì— í•„ìˆ˜ì ì¸ ìš”ì†Œ
  - ë£¨í”„, if/else ë¸”ë¡, ì‚¼í•­ì‹... ë¶„ê¸°ê°€ ì—†ëŠ” ì‹¤ì œ ì½”ë“œëŠ” ìƒìƒí•˜ê¸° ì–´ë ¤ì›€.
- ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ìš”ì†ŒëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê°€ì¥ í° ë¹„ìš© ì¤‘ í•˜ë‚˜ê°€ ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
  - ìµœì‹  í•˜ë“œì›¨ì–´ëŠ” íŒŒì´í”„ë¼ì´ë‹ì„ í†µí•´ ì´ì „ ëª…ë ¹ì–´ê°€ ì²˜ë¦¬ë˜ëŠ” ë™ì•ˆ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì½ê³  ë””ì½”ë”©ì„ ì‹œì‘í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì†ë„ê°€ í¬ê²Œ í–¥ìƒ.
  - ë¬¼ë¡  ì´ëŠ” ë‹¤ìŒ ëª…ë ¹ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì•Œê³  ìˆëŠ” í•˜ë“œì›¨ì–´ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. ë¶„ê¸°ê°€ ì—†ëŠ” ê²½ìš°ì—ëŠ” ì‹œí€€ìŠ¤ì—ì„œ ë‹¤ìŒì— ì˜¤ëŠ” ëª…ë ¹ì–´ê°€ ë¬´ì—‡ì´ë“  ê°„ë‹¨í•©ë‹ˆë‹¤.
  - ë¶„ê¸°ê°€ ìˆëŠ” ê²½ìš° CPUëŠ” ë¶„ê¸° ì˜ˆì¸¡ê¸°(branch predictors)ë¼ëŠ” ë‚´ì¥ ê¸°ëŠ¥ì„ í†µí•´ ë‹¤ìŒ ëª…ë ¹ì–´ ì˜ˆì¸¡
    - ë³´í†µì€ ë§ì•„ ë–¨ì–´ì§€ì§€ë§Œ ì–˜ì¸¡ì´ í‹€ë¦¬ëŠ” ê²½ìš°ì—ëŠ” í° ë¹„ìš©ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.
- ë”°ë¼ì„œ dotnet8 ì»´íŒŒì¼ëŸ¬ëŠ” ë¸Œëœì¹˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ë…¸ë ¥